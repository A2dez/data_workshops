---
title: "Dublin Data Science Workshop on Survival Analysis"
author: "Mick Cooney <mickcooney@gmail.com>"
date: "Monday, 24 September 2018"
output:
  html_document:
    fig_caption: yes
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float:
      smooth_scroll: FALSE
  pdf_document: default
---

```{r knit_opts, include = FALSE}
knitr::opts_chunk$set(tidy  = FALSE
                     ,cache = FALSE
                     ,message = FALSE
                     ,warning = FALSE
                     ,fig.height =  8
                     ,fig.width  = 11)

library(tidyverse)
library(scales)
library(cowplot)
library(survival)
library(survminer)
library(muhaz)
library(broom)


options(width = 80L
       ,warn  = 1
        )

set.seed(42)

source('data_setup.R')
```



# Basic Concepts

In *survival analysis* we analyse *time-to-event* data and try to estimate the
underlying distribution of times for events to occur.

## Introduction

For the purposes of this workshop, we focus on a single event type, but the
topic is wide and broadly applicable.


### Workshop Materials

All materials for this workshop is available in my standard GitHub repo:

https://github.com/kaybenleroll/dublin_r_workshops


![book cover](img/asaur_cover.jpg)


The content of this workshop is partly based on the book "Applied Survival
Analysis Using R" by Dirk F. Moore. The data from this book is available from
CRAN via the package `asaur` and there is a GitHub repo for the
code in the book also:

https://github.com/kolaczyk/sand


### Example Datasets

This workshop will use a number of different time-to-event datasets, so we
look at those first.

#### Telco Churn Data

The telco churn data contains some customer usage information on mobile phone
customers and whether or not they left their subscription accounts.

```{r show_telco_churn_data, echo=TRUE}
telco_churn_tbl %>% glimpse()

telco_churn_tbl %>% head()
```


#### Prostate Survival

The prostate survival data is generated data from a cancer survival study.

```{r show_prostate_data, echo=TRUE}
prostate_surv_tbl %>% glimpse()

prostate_surv_tbl %>% head()
```

#### Smoker Data

The smoker data is a smoking relapse study involving a number of different
treatments and details on when and if the patient resumed smoking.

```{r show_smoker_data, echo=TRUE}
pharmaco_smoker_tbl %>% glimpse()

pharmaco_smoker_tbl %>% head()
```



## Basic Principles  

### Data Censoring and Truncation

In the problems we work on, our data observation process does not allow us to
fully observe the variable in question. Instead, our observations are often
*right-censored* - that is, we know the value in question is at least the
observed value, but it may also be higher.

Expressing this formally, suppose $T^*$ is the time to event, and $U$ is the
time to the censoring, the our observed time $T$, and censoring indicator,
$\delta$, are given by

\begin{eqnarray*}
T      &=& \min(T^*, U) \\
\delta &=& I[T^* \leq U]
\end{eqnarray*}


A less common phenomenon is *left-censoring* - where we observe the event to be
at most a given duration. This may happen in medical studies where continual
observation of the patients is not possible or feasible.

### Hazard and Survival Functions

Our key goal is to find the survival distribution - the distribution of times
from some given start point to the time of the event.

Two common ways of specifying this distribution are the *survival function*,
$S(t)$ and the *hazard function*, $h(t)$.

$S(t)$ is the probability of surviving to time $t$, so is defined as follows:

$$
S(t) = P(T > t), \, 0 \leq t \leq \infty
$$

Thus, $S(0) = 1$ and decreases monotonically with time $t$. As it is a
probability it is non-negative.

We can also define the survival function in terms of the instantaneous failure
rate, the probability of failing at exactly time $t$. More formally,

$$
\lambda(t) = \lim_{\delta \to 0} \frac{P(t \leq T \leq t + \delta \; | \, T > t)}{\delta}
$$

This is also called the *intensity function* or the *force of mortality*


### Cumulative Functions

Some analyses make it easier to use the *cumulative hazard function* 

$$
\Lambda(t) = \int^t_0 \lambda(u) \, du
$$

As a consequence of this definition we also have

$$
S(t) = \exp(- \Lambda(t))
$$

### Mean and Median Survival

Finally, two common statistics for survival are the *mean survival time* and
the *median survival time*:

The mean survival time is the expected value of the distribution, using
standard probability definitions:

$$
\mu = E(T) = \int^\infty_0 t f(t) \, dt = \int^\infty_0 S(t) \, dt
$$

The median survival time, $t_{\text{med}}$ such that

$$
S(t_{\text{med}}) = 0.5
$$


## Example Distributions

Let's look at some different survival distributions.

### Constant Hazard

```{r constant_hazard_plots, echo=TRUE}
t_seq <- 1:100

h_const_seq <- rep_along(t_seq, 0.01)
S_const_seq <- cumprod(c(1, (1 - h_const_seq)))


hazard_plot <- ggplot() +
    geom_line(aes(x = t_seq, y = h_const_seq)) +
    expand_limits(y = 0) +
    xlab("Time") +
    ylab("Instantaneous Hazard")

cuml_plot <- ggplot() +
    geom_line(aes(x = c(0, t_seq), y = S_const_seq)) +
    expand_limits(y = 0) +
    xlab("Time") +
    ylab("Cumulative Survival")

plot_grid(hazard_plot, cuml_plot, nrow = 2)
```

If we increase the constant hazard, we get a similar shape, but the curve
declines faster.

```{r constant_hazard_higher_plots, echo=TRUE}
h_const_high_seq <- rep_along(t_seq, 0.03)
S_const_high_seq <- cumprod(c(1, (1 - h_const_high_seq)))


hazard_plot <- ggplot() +
    geom_line(aes(x = t_seq, y = h_const_high_seq)) +
    geom_line(aes(x = t_seq, y = h_const_seq)
             ,colour = 'blue', linetype = 'dashed') +
    expand_limits(y = 0) +
    xlab("Time") +
    ylab("Instantaneous Hazard")

cuml_plot <- ggplot() +
    geom_line(aes(x = c(0, t_seq), y = S_const_high_seq)) +
    geom_line(aes(x = c(0, t_seq), y = S_const_seq)
             ,colour = 'blue', linetype = 'dashed') +
    expand_limits(y = 0) +
    xlab("Time") +
    ylab("Cumulative Survival")

plot_grid(hazard_plot, cuml_plot, nrow = 2)
```


### Early Hazard

```{r early_hazard_plots, echo=TRUE}
early_seq <- seq(0.10, 0.01, by = -0.01)
late_seq  <- rep(0.01, 100 - length(early_seq))

h_early_seq <- c(early_seq, late_seq)
S_early_seq <- cumprod(c(1, (1 - h_early_seq)))


hazard_plot <- ggplot() +
    geom_line(aes(x = t_seq, y = h_early_seq)) +
    expand_limits(y = 0) +
    xlab("Time") +
    ylab("Instantaneous Hazard")

cuml_plot <- ggplot() +
    geom_line(aes(x = c(0, t_seq), y = S_early_seq)) +
    expand_limits(y = 0) +
    xlab("Time") +
    ylab("Cumulative Survival")

plot_grid(hazard_plot, cuml_plot, nrow = 2)
```



### Late Hazard

```{r late_hazard_plots, echo=TRUE}
late_seq  <- seq(0.01, 0.10, by = 0.0015)
early_seq <- rep(0.01, 100 - length(late_seq))

h_late_seq <- c(early_seq, late_seq)
S_late_seq <- cumprod(c(1, (1 - h_late_seq)))


hazard_plot <- ggplot() +
    geom_line(aes(x = t_seq, y = h_late_seq)) +
    expand_limits(y = 0) +
    xlab("Time") +
    ylab("Instantaneous Hazard")

cuml_plot <- ggplot() +
    geom_line(aes(x = c(0, t_seq), y = S_late_seq)) +
    expand_limits(y = 0) +
    xlab("Time") +
    ylab("Cumulative Survival")

plot_grid(hazard_plot, cuml_plot, nrow = 2)
```


# Estimations of the Survival Functions

## The Kaplan-Meier Estimator

Kaplan-Meier is the standard method for estimating the survival function of a
given dataset. Formally, it is defined as follows

$$
\hat{S}(t) = \prod_{t_i \leq t} (1 - \hat{q}_i) = \prod_{t_i \leq t} \left(1 - \frac{d_i}{n_i}\right)
$$

where $n_i$ is the number of subjects at risk at time $t$, and $d_i$ is the
number of individuals who fail at that time.

### Using Kaplan-Meier

In R, we construct KM estimators using the `survfit()` function.

Before we move on to our datasets, we start with a small set of data.

```{r km_estimator_intro, echo=TRUE}
tt   <- c(7, 6, 6, 5, 2, 4)
cens <- c(0, 1, 0, 0, 1, 1)

Surv(tt, cens)

sample_tbl <- data_frame(tt = tt, cens = cens)

example_km <- survfit(Surv(tt, cens) ~ 1, data = sample_tbl, conf.type = 'log-log')

plot(example_km)
```

Basic plotting routines are worth trying, but the `survminer` package has
specialised plots that use `ggplot2` to create them.

```{r km_estimator_survminer, echo=TRUE}
ggsurvplot(example_km)
```

Printing out the 'fitted' object gives us some basic statistics:

```{r km_estimator_print, echo=TRUE}
example_km %>% print()
```

We get more details from the `summary()` function:

```{r km_estimator_summary, echo=TRUE}
example_km %>% summary()
```

#### Exercises

  1. Construct the KM estimator for the telco churn data
  1. What is the median survival time for this data?
  1. What is the mean survival time?
  1. Repeat the above for the other two datasets.


### Follow-up Time

A useful measure may be how long the observation period lasts, something that
can be subtly difficult to measure.

One method is to switch the censoring labels - that is, we consider the
original event as a censoring of the observation in the study.

```{r measure_followup_time, echo=TRUE}
sample_tbl <- sample_tbl %>%
    mutate(follow = 1 - cens)

follow_km <- survfit(Surv(tt, follow) ~ 1, data = sample_tbl, conf.type = 'log-log')

ggsurvplot(follow_km)

follow_km %>% summary()
```


## Smoothed Hazard Functions

An empirical estimate of the hazard function is given by

$$
\mu(i) = \frac{d_i}{n_i}
$$

This is a very noisy estimate as it is sensitive to sample noise.

### The Hazard Estimator

To obtain more smooth functions we use kernel density estimator techniques.

```{r show_muhaz_plots, echo=TRUE}
sample_muhaz <- muhaz(sample_tbl$tt, sample_tbl$cens, max.time = 7)

plot(sample_muhaz)
```

`broom` has tidying methods for `muhaz()` and this allows us to create plots
with `ggplot2`

```{r show_muhaz_tidy, echo=TRUE}
muhaz_tidy_tbl <- sample_muhaz %>% tidy()

muhaz_tidy_tbl %>% glimpse()
```

We have estimates of the hazard function now and so can plot it.

```{r show_muhaz_tidy_plot, echo=TRUE}
ggplot(muhaz_tidy_tbl) +
    geom_line(aes(x = time, y = estimate)) +
    expand_limits(x = 0, y = 0) +
    xlab("Time") +
    ylab("Estimated Hazard")
```


#### Exercises

  1. Construct the smoothed estimator for the telco churn data
  1. What time has the highest hazard rate?
  1. What time has the lowest hazard rate?
  1. Repeat the above for the other two datasets.


### Boundary Corrections

By default, `muhaz()` corrects for the boundary on both sides, but we may not
wish this. To get estimates without this correction, we add it as an argument
to the function call.


```{r show_muhaz_nocorr, echo=TRUE}
sample_nocorr_muhaz <- muhaz(sample_tbl$tt, sample_tbl$cens, max.time = 7
                            ,b.cor = 'none')

plot(sample_nocorr_muhaz)


sample_nocorr_muhaz_tidy_tbl <- sample_nocorr_muhaz %>% tidy()

ggplot(sample_nocorr_muhaz_tidy_tbl) +
    geom_line(aes(x = time, y = estimate)) +
    expand_limits(x = 0, y = 0) +
    xlab("Time") +
    ylab("Estimated Hazard")
```

To help ensure these smoothed estimates are capturing the correct aspects of
the data, we also have the equivalent `pehaz()` functions - giving histogram
estimates of the hazards, much how histogram and kernel estimates are discrete
and continuous analogies of one another.

#### Exercises

  1. Construct the non-corrected smoothed estimator for the telco churn data
  1. Repeat the above for the other two datasets.


## Comparing KM and Smoothed Estimates

Now that we have both methods of checking the empirical estimates, it is good
to compare them.

Before we do this, we note that Kaplan-Meier estimates the survival function
but `muhaz()` gives us estimates of the hazard function.

Thus we need to do some conversions and the easiest way to do this is to
numerically integrate the hazard functions to calculate the survival function
and then compare to the KM estimate.

```{r integrate_muhaz, echo=TRUE}
muhaz_surv_tbl <- muhaz_tidy_tbl %>%
    mutate(dt  = c(0, diff(time))
          ,S_t = 1 - cumsum(estimate * dt))

ggsurvplot(example_km)$plot +
    geom_line(aes(x = time, y = S_t), data = muhaz_surv_tbl)
```


### No Boundary Correction

We also want to compare the results without the boundary correction.

```{r compare_nocorr_estimates, echo=TRUE}
muhaz_nocorr_surv_tbl <- sample_nocorr_muhaz_tidy_tbl %>%
    mutate(dt  = c(0, diff(time))
          ,S_t = 1 - cumsum(estimate * dt))

ggsurvplot(example_km)$plot +
    geom_line(aes(x = time, y = S_t), colour = 'blue', data = muhaz_surv_tbl) +
    geom_line(aes(x = time, y = S_t), data = muhaz_nocorr_surv_tbl)
    

```

We see that removing the boundary corrections can cause big discrepancies
between the smoothed and discrete estimates (at least in this case)





# R Environment

```{r show_session_info, echo=TRUE, message=TRUE}
devtools::session_info()
```
