---
title: "Dublin Data Science Workshop on Time Series Analysis"
author: "Mick Cooney <mickcooney@gmail.com>"
date: "Monday, 21 January 2019"
output:
  html_document:
    fig_caption: yes
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float:
      smooth_scroll: FALSE
  pdf_document: default
---

```{r import_packages, echo=FALSE}
library(quantmod)
library(PerformanceAnalytics)


import::from(magrittr   ,"%>%", set_colnames)
import::from(tibble     ,as_tibble, tibble, tribble, add_column, glimpse)
import::from(readr      ,read_csv, read_tsv, write_csv, cols, col_character
                        ,col_date, col_number)
import::from(dplyr      ,mutate, mutate_all, mutate_if, filter, select
                        ,group_by, left_join, inner_join, rename, summarise
                        ,summarise_at, summarise_all, vars, funs, distinct
                        ,sample_n, top_n, pull, if_else, count, one_of
                        ,arrange, bind_rows)
import::from(tidyr      ,gather, spread, nest, unnest)
import::from(ggplot2    ,ggplot, aes, xlab, ylab, ggtitle
                        ,geom_histogram, geom_boxplot, geom_bar, geom_col
                        ,geom_tile, geom_vline, geom_line, geom_smooth
                        ,geom_point
                        ,scale_x_continuous, scale_y_continuous
                        ,scale_x_discrete, scale_y_discrete
                        ,scale_fill_continuous, scale_fill_discrete
                        ,expand_limits, theme, element_text, facet_grid
                        ,facet_wrap, theme_set, theme)
import::from(scales     ,comma)
import::from(zoo        ,yearmon)


import::from(timetk     ,tk_tbl, tk_ts, tk_xts)
import::from(cowplot    ,theme_cowplot, plot_grid)
import::from(cranlogs   ,cran_downloads)
import::from(tidyquant  ,tq_mutate, tq_transmute, tq_mutate_xy)
import::from(xts        ,apply.daily, apply.weekly, apply.monthly
                        ,apply.quarterly, apply.yearly)

import::from('custom_functions.R', retrieve_cran_download_data)
```


```{r knit_opts, include = FALSE}
knitr::opts_chunk$set(tidy  = FALSE
                     ,cache = FALSE
                     ,message = FALSE
                     ,warning = FALSE
                     ,fig.height =  8
                     ,fig.width  = 11)

options(width = 80L
       ,warn  = 1
        )

set.seed(42)

theme_set(theme_cowplot())
```



# Introduction

Time series occur in most fields of study that produce quantitative data.
Whenever quantities are measured over time, those measurements form a
time-series, or more formally, a discrete-time stochastic process.

## Workshop Materials

All materials for this workshop is available in my standard GitHub repo:

https://github.com/kaybenleroll/dublin_r_workshops


![](img/itswr_cover.jpg)


The content of this workshop is partly based on the book "Introductory Time
Series with R" by Paul Cowpertwait and Andrew Metcalfe. The data from this book
is available from the website:

http://www.maths.adelaide.edu.au/emac2009/


### R Packages

In this workshop we focus on the use of 'tidy'-style tools in the analysis of
time-series. In particular we look at packages such as `tidyquant` that enable
and simplify this approach to time-series analysis.


## Basic Concepts

A famous example of a time-series is count of airline passengers in the US,
as shown in the figure below. This is a simple univariate time-series, with
measurements taken on a monthly basis over a number of years, each datum
consisting of a single number - the number of passengers travelling via
a commercial flight that month.

```{r plot_air_passenger_timeseries, echo=TRUE}
plot(AirPassengers)
```

Before we begin analysing such data, we first need to create a mathematical
framework to work in. Fortunately, we do not need anything too
complicated, and for a finite time-series of length $N$, we model the time
series as a sequence of $N$ random variables, $X_i$, with $i = 1, 2, ..., N$.

Note that each individual $X_i$ is a wholly separate random variable: we only
ever have a single measurement from each random variable. In many cases we
simplify this, but it is important to understand and appreciate that such
simplifications are just that. Time series are difficult to analyse.

Before we get to any of that though, and before we try to build any kind of
models for the data, we always start with visualising the data. Often, a simple
plot of the data helps use pick out aspects to analyse and incorporate into the
models. For time series, one of the first things to do is the *time plot*, a
simple plot of the data over time.

For the passenger data, a few aspects stand out that are very common in time
series. It is apparent that the numbers increase over time, and this systematic
change in the data is called the *trend*. Often, approximating the trend as a
linear function of time is adequate for many data sets.

A repeating pattern in the data that occurs over the period of the data (in
this case, each year), is called the *seasonal variation*, though a more
general concept of 'season' is implied --- it often will not coincide with the
seasons of the calendar.

A slightly more generalised concept from the seasonality is that of *cycles*,
repeating patterns in the data that do not correspond to the natural fixed
periods of the model. None of these are apparent in the air passenger data, and
accounting for them are beyond the scope of this introductory tutorial.

Finally, another important benefit of visualising the data is that it helps
identify possible *outliers* and *erroneous* data.

In many cases, we will also be dealing with time series that have multiple
values at all, many or some of the points in time.

Often, these values will be related in some ways, and we will want to analyse
those relationships also. In fact, one of the most efficient methods of
prediction is to find *leading indicators* for the value or values you wish to
predict --- you can often use the current values of the leading indicators to
make inference on future values of the related quantities.

The fact that this is one of the best methods in time series analysis says a
lot about the difficulty of prediction (Yogi Berra, a US baseball player noted
for his pithy statements, once said "Prediction is difficult, especially about
the future").


## Example Timeseries

In this workshop we will look at a number of different time-series, discussed
here.

This data comes in a few different format, and this workshop discusses methods
for analysing this data in a common format.


### Air Passenger Data

As mentioned previously, a canonical time-series is the airline passenger
dataset, and this is the first dataset we look at.

```{r show_airline_passengers}
AirPassengers %>% print()

AirPassengers %>% plot()
```

In this workshop we will convert all time series into the tibbles: the package
`timetk` allows us to do this.

```{r convert_airpassengers_tibble, echo=TRUE}
airpassengers_tbl <- AirPassengers %>% tk_tbl(rename_index = 'month')

airpassengers_tbl %>% print()


ggplot(airpassengers_tbl) +
    geom_line(aes(x = month, y = value)) +
    xlab('Date') +
    ylab('Passenger Count') +
    expand_limits(y = 0) +
    ggtitle('Plot of Air Passenger Time Series')
```


### Maine Unemployment

Time series are very common in econometrics, and a dataset provided in the
text is that of monthly unemployment statistics in Maine from 1996 on. I have
included the datafile in this workshop.

```{r load_maine_unemployment, echo=TRUE}
maine_ts <- scan('data/Maine.dat', skip = 1) %>%
    ts(start = 1996, frequency = 12)

maine_ts %>% print()
maine_ts %>% plot()
```

As before, we convert this data into a tibble and recreate the plot using
`ggplot2`.

```{r maine_unemployment_tibble, echo=TRUE}
maine_tbl <- maine_ts %>% tk_tbl(rename_index = 'month')

maine_tbl %>% print()


ggplot(maine_tbl) +
    geom_line(aes(x = month, y = value)) +
    xlab('Date') +
    ylab('Unemployment Numbers') +
    expand_limits(y = 0) +
    ggtitle('Plot of Maine Unemployment Time Series')
```


### Australian Consumption Statistics

Governments produce regular data on consumption numbers for their economy. One
such dataset is contained in the file `cbe.dat`, containing data of chocolate,
beer and energy production on a monthly basis.

```{r load_australian_cbe_data, echo=TRUE}
cbe_raw_tbl <- read_tsv('data/cbe.dat')

cbe_raw_tbl %>% glimpse()
```

Similar to the Maine file, this data does not contain time indices for the
data. For the sake of completeness, we use the same approach as before and
convert to a tibble, but we will also show how to construct the time index
without having to do intermediate conversions.

First we add time indices via `ts` conversions.


```{r construct_cbe_timeseries, echo=TRUE}
cbe_ts <- cbe_raw_tbl %>%
    as.matrix() %>%
    ts(start = 1958, frequency = 12)

cbe_ts %>% head(10)
cbe_ts %>% plot()
```

An alternative approach is to add the time index directly.

```{r australian_cbe_add_time, echo=TRUE}
n_data <- cbe_raw_tbl %>% nrow()

cbe_tbl <- cbe_raw_tbl %>%
    add_column(month = (1958 + (0:(n_data-1)/12)) %>% yearmon, .before = 1)


cbe_tbl %>% glimpse()
cbe_tbl %>% print()
```

Having constructed the tibble, we now construct these time series plots using
`ggplot2`.

```{r construct_cbe_plots, echo=TRUE}
plot_tbl <- cbe_tbl %>%
    gather('product', 'value', -month)

ggplot(plot_tbl) +
    geom_line(aes(x = month, y = value, colour = product)) +
    expand_limits(y = 0) +
    xlab("Date") +
    ylab("Production Amount") +
    scale_y_continuous(labels = comma) +
    ggtitle('Production Data from Australian Government')
```


Due to the different scales, it might be more useful to use a faceted plot for
each product:

```{r plot_cbe_facetted_data, echo=TRUE}
plot_tbl <- cbe_tbl %>%
    gather('product', 'value', -month)

ggplot(plot_tbl) +
    geom_line(aes(x = month, y = value)) +
    facet_grid(rows = vars(product), scales = 'free_y') +
    expand_limits(y = 0) +
    xlab("Date") +
    ylab("Production Amount") +
    scale_y_continuous(labels = comma) +
    ggtitle('Production Data from Australian Government')
```


### CRAN Package Downloads

An interesting dataset is the daily count of package downloads from CRAN. This
data is easy to obtain via use of the package `cranlogs`, which gives us use
of the `cran_downloads()` function.

For this workshop, we will look at some of the main packages that comprise
the 'tidyverse', as well as the total number of downloads from CRAN.

```{r download_cran_data, echo=TRUE}
cran_data_file <- 'data/cran_download_data.csv'

if(file.exists(cran_data_file)) {
    cran_data_tbl <- read_csv(cran_data_file)
} else {
    cran_pkgs <- c('dplyr', 'tidyr', 'ggplot2', 'lubridate', 'stringr', 'tibble'
                  ,'broom', 'jsonlite', 'purrr', 'readr', 'tidyquant')
      

    cran_data_tbl <- retrieve_cran_download_data(cran_pkgs, '2014-01-01', '2018-12-31')
    
    write_csv(cran_data_tbl, path = cran_data_file)
}


cran_data_tbl %>% glimpse()
cran_data_tbl %>% print()
```

First we construct a simple line plot of the download counts, facetted by
package.

```{r plot_cran_downloads, echo=TRUE}
ggplot(cran_data_tbl) +
    geom_line(aes(x = date, y = count)) +
    facet_wrap(vars(package), scales = 'free_y') +
    expand_limits(y = 0) +
    scale_y_continuous(labels = comma) +
    ggtitle('Facetted Lineplots of CRAN Daily Downloads')
```

Not all packages have download data as some packages were created after the
start of our observation period. This manifests as zero counts for that
package. We discuss these issues later on in the workshop.


## Combining Time Series

Useful insights are often gained from combining different datasets together.

Looking at our datasets, one possible interesting relationship is that between
energy production and airline passengers -- it is reasonable to expect that
both of these quantities will be related as they are related to the overall
health and size of the economy.

A major benefit of using tidy tools for time series is to make such data
manipulation and arrangement simple: combining datasets is simply a matter
of using the two-table joins.

To illustrate, we combine the Air Passenger data with the Australian economic
data, using the following code. Note that we rename the air passenger data
at the end to make it more meaningful and useful.


```{r combining_airpassengers_australia_data, echo=TRUE}
ap_econ_combined_tbl <- airpassengers_tbl %>%
    left_join(cbe_tbl, by = 'month') %>%
    filter(complete.cases(.)) %>%
    rename(air = value)


ap_econ_combined_tbl %>% glimpse()
ap_econ_combined_tbl %>% print()
```

We return to this dataset later in the workshop.


# Manipulation of Time Series Data

Much like all data, it is rate to get time-series exactly in the format we
want for analysis. For various reasons, we may want to analyse transformations
or aggregations of this data.

Much like feature engineering and variable selection, this process can be
more art than science - there are no hard and fast rules, merely principles
and rules-of-thumb.

The last few years in particular have seen the development of a number of
tools to aid us with the analysis of time series along 'tidy' principles. In
particular, we will focus on the use of `tidyquant` - a package aimed at
analysing financial data, but which is also very useful for time series.


## Aggregating Data

From a conceptual point of view, aggregating time series is the most
straightforward - we group the data by longer periods of time and aggregate
each individual 'bucket' of data as desired or required.

As an example of this, suppose we wish to look at the air passenger data as an
annual sum for each year. Our data is monthly, so we need to aggregate this
up into annual numbers.


```{r aggregate_ap_data_annual, echo=TRUE}
ap_yearly_dplyr_tbl <- airpassengers_tbl %>%
    group_by(year = month %>% format('%Y') %>% as.numeric) %>%
    summarise(ann_total = sum(value))


ap_yearly_dplyr_tbl %>% glimpse()
ap_yearly_dplyr_tbl %>% print()
```

The above transformation was straightforward using existing `dplyr`
functionality, but we can also use routines provided for by `tidyquant` and its
function `tq_transmute`:


```{r aggregate_ap_annual_transmute, echo=TRUE}
ap_yearly_tidyquant_tbl <- airpassengers_tbl %>%
    tq_transmute(
        select     = value
       ,mutate_fun = apply.yearly
       ,FUN        = sum
       ,na.rm      = TRUE
       ,col_rename = 'ann_total'
    )


ap_yearly_tidyquant_tbl %>% glimpse()
ap_yearly_tidyquant_tbl %>% print()
```



# Exploratory Data Analysis of Time Series

The first step in all exploratory analysis is simple visualisation: simple
lines plots such as those we have seen are our starting point. The human
brain is excellent at pattern recognition, so a simple plot often guides our
analysis more effectively than a suite of numerical diagnostics.

## Air Passenger Data

We start with the air passenger data, and remind ourselves of the simple line
plot of the value over time.


```{r plot_airpassenger_exploration, echo=TRUE}
ggplot(airpassengers_tbl) +
    geom_line(aes(x = month, y = value)) +
    xlab('Date') +
    ylab('Passenger Count') +
    expand_limits(y = 0) +
    ggtitle('Plot of Air Passenger Time Series')
```

This plot suggests a strong seasonal effect as well as a trend and it is worth
exploring this.

We can explore the underlying trend of this data by looking at the yearly
totals.

```{r plot_ap_yearly_totals, echo=TRUE}


```






# Time Series Decomposition

Many time series are dominated by trends or seasonal effects, and we can
create fairly simple models based on these two components. The first of these,
the *additive decompositional model*, is just the sum of these effects, with
the residual component being treated as random:

$$
x_t = m_t + s_t + z_t,
$$

where, at any given time $t$,

\begin{eqnarray*}
x_t && \text{is the observed value} \\
m_t && \text{is the trend} \\
s_t && \text{is the seasonal component} \\
z_t && \text{is the error term}
\end{eqnarray*}


It is worth noting that, in general, the error terms will be a correlated
sequence of values, something we will account for and model later.

In other cases, we could have a situation where the seasonal effect
increases as the trend increases, modeling the values as:

$$
x_t = m_t s_t + z_t.
$$

Other options also exist, such as modeling the log of the observed values,
which does cause some non-trivial modeling issues, such as biasing any
predicted values for the time series.

Various methods are used for estimating the trend, such as taking a
*moving average* of the values, which is a common approach.



\worksheetexercise
Using the \texttt{decompose()} function in R, look at the trend and
the seasonal variation for the airline passenger data. The output of
this function can be plotted directly, and visually check the
output. Does the output match your intuition about what you observed?

\worksheetexercise
Repeat this process for the CBE dataset.

\worksheetexercise
Try a multiplicative model for all of the above. \texttt{decompose()}
allows the selection of this via the `\texttt{type}' parameter. Is the
multiplicative model better? In either case, explain why this might be.

\worksheetexercise
Repeat the above, but use the \texttt{stl()} R function instead of
\texttt{decompose()}. Compare the output of the two.



# Autocorrelation


Assuming we can remove the trend and the seasonal variation, that
still leaves the random component, $z_t$. Unfortunately, analysing
this is usually highly non-trivial. As discussed, we model the random
component as a sequence of random variables, but no further
assumptions we made.

To simplify the analysis, we often make assumptions like
\emph{independent and identically distributed (i.i.d.)} random
variables, but this will rarely work well. Most of the time, the $z_t$
are correlated.

The \emph{expected value} or \emph{expectation} of a random variable
$x$, denoted $E(x)$, is the mean value of $x$ in the population. So,
for a continuous $x$, we have

$$
\mu = E(x) = \int p(x) \, x \, dx.
$$


and the \emph{variance}, $\sigma^2$, is the expectation of the squared
deviations,

$$
\sigma^2 = E[(x - \mu)^2],
$$

For bivariate data, each datapoint can be represented as $(x, y)$ and
we can generalise this concept to the \emph{covariance},
$\gamma(x, y)$,

$$
\gamma(x, y) = E[(x - \mu_x)(y - \mu_y)].
$$


Correlation, $\rho$, is the standardised covariance, dividing the
covariance by the standard deviation of the two variables,

$$
\rho(x, y) = \frac{\gamma(x, y)}{\sigma_x \sigma_y}.
$$


The mean function of a time series model is

$$
\mu(t) = E(x_t),
$$


with the expectation in this case being across the \emph{ensemble} of
possible time series that might have been produced by this model. Of
course, in many cases, we only have one realisation of the model, and
so, without any further assumption, estimate the mean to be the
measured value.

If the mean function is constant, we say that the time-series is
\emph{stationary in the mean}, and the estimate of the population mean
is just the sample mean,

$$
\mu = \sum^n_{t=1} x_t.
$$


The variance function of a time-series model that is stationary in the
mean is given by

$$
\sigma^2(t) = E[(x_t - \mu)^2],
$$


and if we make the further assumption that the time-series is also
stationary in the variance, then the population variance is just the
sample variance

$$
\text{Var}(x) = \frac{\sum(x_t - \mu)^2}{n - 1}
$$


Autocorrelation, often referred to as \emph{serial correlation}, is
the correlation between the random variables at different time
intervals. We can define the \emph{autocovariance function} and the
\emph{autocorrelation function} as functions of the \emph{lag}, $k$, as

\begin{eqnarray}
\gamma_k &=& E[(x_t - \mu)(x_{t+k} - \mu)], \\
\rho_k   &=& \frac{\gamma_k}{\sigma^2}.
\end{eqnarray}


Be default, the \texttt{acf()} function plots the \emph{correlogram},
which is a plot of the sample autocorrelation at $r_k$ against the lag
$k$.

\worksheetexercise
Using the function \texttt{acf()}, calculate the autocorrelations for
all the time series we have looked at. Look at the structure of the
output, and use the help system to see what options are provided.

\worksheetexercise
Check the output of \texttt{acf()} against manual calculations of the
correlations at various timesteps. Do the numbers match?

\noindent
\textbf{HINT:} The \texttt{cor()} function and some vector indexing
will be helpful here.

\worksheetexercise
Plot the output of the \texttt{acf()} for the different time
series. Think about what these plots are telling you. Do do these
plots help the modelling process, if so, how?

\worksheetexercise
Decompose the air passenger data and look at the appropriate
correlogram. What does this plot tell you? How does it differ from the
previous correlogram you looked at?

\worksheetexercise
How can we use all that we have learned so far to assess the efficacy
of the decompositional approach for time series?



# Basic Forecasting

As mentioned earlier, an efficient way to forecast a variable is to
find a related variable whose value leads it by one or more
timesteps. The closer the relationship and the longer the lead time,
the better it becomes.

The trick, of course, is to find a leading variable.

Multivariate series has a temporal equivalent to correlation and
covariance, known as the \emph{cross-covariance function (ccvf)} and
the \emph{cross-correlation function (ccf)},

\begin{eqnarray}
\gamma_k(x, y) &=& E[(x_{t+k} - \mu_x)(y_t - \mu_y)], \\
\rho_k(x, y)   &=& \frac{\gamma_k(x, y)}{\sigma_x \sigma_y}.
\end{eqnarray}


Note that the above functions are not symmetric, as the lag is always
on the first variable, $x$.


\worksheetexercise
Load the building approvals and activity data from the
\texttt{ApprovActiv.dat} file. The data is quarterly and starts in
1996. Determine which is the leading variable and investigate the
relationship between the two.

\worksheetexercise
Binding the time-series using \texttt{ts.union()}, find the
cross-correlations for the building data. Is the relationship
symmetric, and why?

\worksheetexercise
Examine the cross-correlations of the random element of the decomposed
time-series for the building data, and compare this to the original
cross-correlations.



Our main objective in forecasting is to estimate the value of a future
quantity, $x_{n+k}$, given past values ${x_1, x_2, ..., x_n}$. We
assume no seasonal or trend effects, or any such effects have been
removed from the data. We assume that the underlying mean of the data
is $\mu_t$, and that this value changes from timestep to timestep, but
this change is random.

Our model can be expressed as

$$
x_t = \mu_t + w_t,
$$

where $\mu_t$ is the non-stationary mean of the process at time $t$
and $w_t$ are independent random variates with mean $0$ and standard
deviation $\sigma$. We let $a_t$ be our estimate of $\mu_t$, and can
define the \emph{exponentially-weighted moving average (EWMA)}, $a_t$
to be

$$
a_t = \alpha x_t + (1 - \alpha) a_{t-1}, \;\;\; 0 \leq \alpha \leq 1.
$$


The value of $\alpha$ controls the amount of smoothing, as is referred
to as the \emph{smoothing parameter}.

\worksheetexercise
Load the data in the \texttt{motororg.dat} file. This is a count of
complaints received on a monthly basis by a motoring organisation from
1996 to 1999. Create an appropriate time series from this data. Plot
the data, checking it for trends or seasonality.

\worksheetexercise
Using the function \texttt{HoltWinters()}, with the additional
parameters set to zero, create the EWMA of the data, allowing the
function itself to choose the optimal value of $\alpha$. Investigate
and visualise the output, comparing it to the original time series.

\worksheetexercise
Specifying values of $\alpha$ of 0.2 and 0.9, create new versions of
the EWMA and compare them with previous fits of the EWMA.


The Holt-Winters method generalises this concept, allowing for trends
and seasonal effects. The equations that govern this model for
seasonal period, $p$, are given by

\begin{eqnarray}
a_t &=& \alpha (x_t - s_{t-p}) + (1 - \alpha)(a_{t-1} - b_{t-1}), \nonumber \\
b_t &=& \beta (a_t - a_{t-1}) + (1 - \beta)b_{t-1},\\
s_t &=& \gamma (x_t - a_t) + (1 - \gamma) s_{t-p}, \nonumber
\end{eqnarray}


where $a_t$, $b_t$, $s_t$ are the estimated level, slope and seasonal
effect at time $t$, and $\alpha$, $\beta$ and $\gamma$ are the
smoothing parameters.

\worksheetexercise
Fit the Holt-Winters parameters to the air passenger data and check
the fit. Visualise the raw time-series against the fitted data.

\worksheetexercise
Predict data ahead for four years and visualise this data. How
reliable are these forecasts do you think?



# Stochastic Methods and Regression

\noindent
A time series $w_t$ is \emph{discrete white noise} if the $w_t$ are
i.i.d with a mean of zero. Thus, they all have the same variance
$\sigma^2$ and $\text{Cor}(w_i, w_j) = 0$ for $i \neq j$. In addition,
if the $w_j \sim N(0, \sigma^2)$ then it is said to be
\emph{Gaussian white noise}.


A time series $x_t$ is a \emph{random walk} if

$$
x_t = x_{t-1} + w_t,
$$


where $w_t$ is a white-noise series.

\worksheetexercise
Generate a white noise series using \texttt{rnorm()}, with an initial
value, $w_0 = 1$. and length 100. Plot the output, and investigate its
correlogram.

\worksheetexercise
Generate a random walk time series with initial value $x_0 = 1$ and
length 100. Plot its output and investigate its correlogram.

\worksheetexercise
Think about how you might create a random walk with an underlying
drift?




The time series $x_t$ is a \emph{auto-regressive process of order $p$},
$\text{AR}(p)$, if,

\begin{equation}
x_t = \sum^p_{i=1} \alpha_i x_{t-i} + w_t,
\end{equation}

\noindent
where $w_t$ is a white-noise process and the $\alpha_p \neq 0$ for an
order-$p$ process.

\worksheetexercise
Generate data for an AR(1) model with $\alpha = 0.5$ and initial value
$x_1 = 1$. Plot the data and investigate its correlogram. Does this
time series appear to be stationary? The R function
\texttt{arima.sim()} can be used for this.

\worksheetexercise
Generate data for an AR(2) model with $\alpha_1 = 1$ and
$\alpha_2 = -0.25$ and initial value $x_1 = 1$, $x_2 = 1$. Plot the
data and investigate its correlogram. Does this time series appear to
be stationary?

\worksheetexercise
Generate data for an AR(2) model with $\alpha_1 = 0.5$ and
$\alpha_2 = 0.5$ and initial value $x_1 = 1$, $x_2 = 1$.

\worksheetexercise
Fit the time-series you generated above to an autoregressive model
using the function \texttt{ar()}. How do the fitted parameters match
the values you used?

\vspace{5mm}

\noindent
A moving average (MA) process of order $q$ is a linear combination of
the current white noise term and the $q$ most recent past white noise
terms,

\begin{equation}
x_t = w_t + \sum^q_{i=1} \beta_i w_{t - i}
\end{equation}

\noindent
where $w_t$ is a white-noise process with mean 0 and variance $\sigma^2$.

\worksheetexercise
Generate data for an MA(1) model with $\beta = 0.5$ and initial value
$x_1 = 1$. Plot the data and investigate its correlogram. Does this
time series appear to be stationary?

\worksheetexercise
Generate data for an MA(2) model with $\beta_1 = 1$ and
$\beta_2 = -0.25$ and initial values $x_1 = 1$, $x_2 = 2$. Plot the
data and investigate its correlogram. Does this time series appear to
be stationary?

\worksheetexercise
Generate data for an MA(2) model with $\beta_1 = 0.5$ and
$\beta_2 = 0.5$ and initial values $x_1 = 1$, $x_2 = 1$.

\worksheetexercise
Fit the time-series you generated above to a moving-average model
using the function \texttt{arima()}. How do the fitted parameters
match the values you used?

\worksheetexercise
Compare the AR and MA models to one another.



%%%
%%% SECTION: ARMA and ARIMA Models
%%%

\worksheetsection{ARMA and ARIMA Models}

\noindent
Now suppose we combine the ideas of autoregressive and moving average
models together. A time series follows an
\emph{autoregressive moving average (ARMA)} process of order $(p, q)$
when

\begin{equation}
x_t = \sum_{i=1}^p \alpha_i x_{t-i} + w_t + \sum_{j=1}^q \beta_j w_{t-j}
\end{equation}

\noindent
where $w_t$ is white noise.

Both $\text{AR}(p)$ and $\text{MA}(q)$ models are special cases of
$\text{ARMA}(p, q)$ (with $q = 0$ and $p = 0$ respectively), and ARMA
models are usually preferred due to \emph{parameter parsimony} ---
when fitting data, the ARMA model is usually more parameter efficient,
requiring few parameters.

\worksheetexercise
Using the R function \texttt{arima.sim()}, create an ARMA(1,1)
time-series of length 1000 with $\alpha = -0.6$, and
$\beta = 0.5$. Plot this time series and check its ACF. Is this
correct?

\worksheetexercise
Using \texttt{arima()}, fit your generated time series to an
$\text{ARMA}(1,1)$ model and compare the fitted output to the values
you have set.

\worksheetexercise
Repeat the above exercises, but for an $\text{ARMA}(2, 2)$ model with
$\alpha_1 = 0.2$, $\alpha_2 = -0.5$, $\beta_1 = -0.1$ and
$\beta_2 = 0.3$.

\worksheetexercise
Investigate the effect of the parameters $p$ and $q$ by generating the
various combinations of the $\text{ARMA}(p, q)$ models but using the
same set of innovations in each case.\\
\textbf{HINT:} \texttt{arima.sim()} has a parameter \texttt{innov = ...}
that allows you to pass in a set of innovations into the ARMA process.

\worksheetexercise
Load in the GBP/NZD currency pair data from the file
\texttt{pound\_nz.dat}, and create a time-series from this. The data is
quarterly, and starts in Q1 1991.

\worksheetexercise
Fit the data GBP/NZD to an $\text{MA}(1)$, an $\text{AR}(1)$ and an
$\text{ARMA}(1, 1)$ process. Which of the above models does the best
job at fitting the data?

\vspace{5mm}

\noindent
It may be becoming quickly apparent that the choice of parameter count
is non-trivial, and some kind of systematic approach would be
desirable.

One such method for choosing the optimal number of parameters is to
fit multiple options and choose the best one, using a metric known as
the \emph{Akaike Information Criterion (AIC)}. The AIC is defined to
be

\begin{equation}
\text{AIC} = -2 \times \text{loglikelihood of fit} + 2 \times
\text{parameter count},
\end{equation}

\noindent
so that it balances the better fitting of parameters while penalising
using too many parameters to fit.

\worksheetexercise
Use the R function \texttt{AIC} to calculate the AIC of the three
models above. Which one is the best? Why is this question a trap?

\worksheetexercise
Using all of the various techniques in this workshop, try to model the
electricity time series data from the CBE dataset.




# R Environment

```{r show_session_info, echo=TRUE, message=TRUE}
detach(package:PerformanceAnalytics)
detach(package:quantmod)
detach(package:TTR)
detach(package:xts)
detach(package:zoo)


devtools::session_info()
```
