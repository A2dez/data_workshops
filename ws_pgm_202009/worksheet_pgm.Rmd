---
title: "Dublin Data Science Workshop on Probabilistic Graphical Models"
author: "Mick Cooney <mickcooney@gmail.com>"
date: "Monday, 14 September 2020"
output:
  html_document:
    fig_caption: yes
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float:
      smooth_scroll: FALSE
  pdf_document: default
---


```{r knit_opts, include = FALSE}
knitr::opts_chunk$set(tidy       = FALSE,
                      cache      = FALSE,
                      message    = FALSE,
                      warning    = FALSE,
                      fig.height =     8,
                      fig.width  =    11)

library(conflicted)
library(tidyverse)
library(magrittr)
library(scales)
library(cowplot)
library(gRain)
library(bnlearn)
library(rbmn)


source("custom_functions.R")

resolve_conflicts(c("magrittr", "rlang", "dplyr", "readr", "purrr", "ggplot2"))


options(
  width    = 80L,
  warn     = 1,
  mc.cores = parallel::detectCores()
  )



set.seed(42)

theme_set(theme_cowplot())
```



---


All code and data for this workshop is available at the following URL:

https://github.com/kaybenleroll/data_workshops

Code is available in the `ws_pgm_202009/` directory.



Content in this workshop is based on the book
[Probabalistic Graphical Models: Principles and Techniques](http://www.springer.com/us/book/9781461422983)
by Soren Hojsgaard.

![](img/graphical_models_cover.png)




Also look at the vignettes for the packages
[`gRain`](https://cran.r-project.org/web/packages/gRain/index.html)
and
[`gRbase`](https://cran.r-project.org/web/packages/gRbase/index.html)


Remember that this topic is massive. I could easily give a full
semester course on this stuff to really do it justice, so most of this
workshop is just me working through the material as I learn it.


As a result, it is highly likely this worksheet and code contains
typos, errors, logical flaws and other mistakes in need of correction
in this workshop, so if you note any, please let me know so I can try
to fix them!


If you want to look into this topic more, there is an old Coursera
course by Daphne Koller (tough going but excellent):

https://www.coursera.org/learn/probabilistic-graphical-models



This course was based on her textbook

[Probabalistic Graphical Models: Principles and Techniques](http://pgm.stanford.edu/)

![](img/pgm_koller_cover.png)

The Gaussian Graphical Models section of the workshop is based on material
found in the book "Bayes Networks With Examples in R" by Scutari and Denis.

![](img/bayesian_network.jpg)



# Basic Concepts

A graph is a mathematical object that can be defined as a pair

$$
\mathcal{G} = (V, E),
$$

where $V$ is a set of *vertices* or *nodes*, and $E$ is a set of *edges* that
joins two vertices. Edges in general may be directed, undirected or bidirected.
They are typically visualised by using shapes or points for the nodes and
lines for the edges.


The concept of *conditional independence* is related to that of
*statistical independence*. Suppose we have three random variables $A$, $B$ and
$C$, then $A$ and $B$ are *conditionally independent* given $C$, written

$$
A \perp B \mid C,
$$

iff, for every given value $c$ in $C$, $A$ and $B$ are independent in the
conditional distribution given $C = c$.


Another way of saying this is that for some $f$ a generic density or
probability mass function, then one characteristic of $A \perp B \, | \, C$
is that

$$
f(a, b \mid c) = f(a \mid c) f(b \mid c).
$$


An equivalent characterisation is that the joint density of $A$, $B$
and $C$ factorises as

$$
f(a, b, c) = g(a, c) \, h(b, c).
$$


Finally, we will also make heavy use of Bayes' Rule, the standard
formula for relating conditional probabilities:

$$
P(A \mid B) = \frac{P(A, B)}{P(B)} = \frac{P(B \mid A) P(A)}{P(B)}.
$$



## Conditional Probability

To explain the concept of conditional probability we first look at some basic
exercises in probability.

In the following questions, we are rolling two six-sided dice (denoted 2D6)
with $D_1$ and $D_2$ denoting the result of the first and second die
respectively, and we denote $T$ as being the sum of the two - i.e.

$$
T = D_1 + D_2
$$



*Question:* We start simple - with no further detail, what is the probability
of the total being 11?

*Answer:* We do not know anything about $D_1$ or $D_2$, so there are two
outcomes that result in $T = 11$:

$$(D_1 = 5, D_2 = 6) \text{ or } (D_1 = 6, D_2 = 5).$$

Thus, the probability of rolling 11 is

$$
P(T = 11) = \frac{2}{36} = 0.05556
$$

---

*Question:* What is the probability of getting 11 if the first dice is 5?

*Answer:* Out total is 11 if the second dice is 6, so there is only 1 possible
outcome of those six possible rolls.

$$
P(T = 11 \mid D_1 = 5) = \frac{1}{6} = 0.1667
$$

### Exercises

We now ask a few questions to ensure you understand these concepts.

  1. If the second dice is a 4, what is the probability the total was 7?
  1. What is the probability of the total being 10 or greater?
  1. If we know the first dice roll is 4, what is the probability of the total
     being 10 or greater?
  1. If we know the second dice roll is 2, what is the probability of the total
     being 10 or greater?
  1. What is the probability of the total being 3 or less?


## Conditional Dependence

Now suppose we know the totals of each dice instead? Suppose we know that
$T = 9$ and we have the following distribution for $D_1$:

$$
P(D_1) = \{0, \, 0, \, 0, \, 0, \, 0.5, \, 0.5 \}
$$
induces the following probability distribution for $D_2$:

$$
P(D_2) = \{0, \, 0, \, 0.5, \, 0.5, \, 0, \, 0 \}
$$

because $D_1 = 5 \implies D_2 = 4$ and $D_1 = 6 \implies D_2 = 3$


Thus, while we conceptually think of $D_1$ and $D_2$ as being independent of
one another, they become dependent if we have knowledge of $T$.

In this case we say that $D_1$ and $D_2$ are *conditionally dependent on one
another given* $T$.


Now we define new variables, $X_1$ and $X_2$:

$$
X_1 =
\begin{cases}
  1 \text{ iff } T \text{ is even}\\
  0 \text{ otherwise}
\end{cases}
\; \; \;
X_2 =
\begin{cases}
  1 \text{ iff } T >= 9\\
  0 \text{ otherwise}
\end{cases}
$$

Given no further information, we say that $X_1$ and $X_2$ are dependent, as
knowing information about one variables affects our knowledge of the other.

However, what happens if we know something about $T$?

In this case, we say that $X_1$ and $X_2$ are *independent given* $T$.



# Bayesian Network

We start with *Bayesian Networks* - where the nodes on the graph represent
discrete random variables.


## The Sprinkler Network

We start with a basic example of a Bayes Network: the sprinkler network.


```{r sprinkler_graph, echo=TRUE}
yn <- c("Yes", "No")

cptable_lst <- list(
  cptable(~Rain,                        levels = yn, values = c(2, 8)),
  cptable(~Sprinkler + Rain,            levels = yn, values = c(1, 99, 4, 6)),
  cptable(~wetGrass + Rain + Sprinkler, levels = yn, values = c(99, 1, 8, 2, 9, 1, 0, 1))
)

sprinkler_cptlist <- cptable_lst %>% compileCPT()
sprinkler_grain   <- sprinkler_cptlist %>% grain()

sprinkler_grain %>% plot()
```


Two events can cause grass to be wet: Either the sprinkler is on or it is
raining. Rain has a direct effect on the use of the sprinkler (namely that when
it rains, the sprinkler is usually not turned on).


This can be modeled with a Bayesian network. The variables (R)ain, (S)prinkler,
Wet(G)rass have two possible values: (y)es and (n)o.


We can factorise the joint probability mass function as

$$
p_{GSR}(g, s, r) = p_{G \mid SR}(g \mid s, r) p_{S \mid R}(s \mid r) p_R(r)
$$

or overloading the notation a little:

$$
P(G, S, R) = P(G \mid S, R) \; P(S, R) = P(G \mid S, R) \; P(S \mid R) \; P(R)
$$


This means we can construct the joint probability table by starting with the
$conditional probability tables$ (CPTs).


Create the 3 CPTs using the `parray()` function and the following conditional
probabilities:

\begin{align*}
P(R)     &= 0.2 \\
P(S|R)   &= 0.01 & P(S|\neg R)    &= 0.4\\
P(G|S,R) &= 0.99 & P(G|S, \neg R) &= 0.9 & P(G|\neg S, R) &= 0.8 & P(G|\neg S, \neg R) &= 0
\end{align*}


First we want to construct this network using our various conditional
probabilities:

```{r sprinkler_conditional_distributions, echo=TRUE}
yn <- c("yes", "no")

## P(R)
p_R <- parray(
  varNames = "Rain",
  levels = list(yn),
  values = c(0.2, 0.8)
)

## P(S|R)
p_S_R <- parray(
  varNames = c("Sprinkler", "Rain"),
  levels   = list(yn, yn), 
  values   = c(0.01, 0.99, 0.4, 0.6)
)

## P(G|S,R)
p_G_SR <- parray(
  varNames = c("GrassWet", "Sprinkler", "Rain"),
  levels   = list(yn, yn, yn),
  values   = c(0.99, 0.01, 0.8, 0.2, 0.9, 0.1, 0, 1)
)

ftable(p_G_SR, row.vars = "GrassWet")
```

We now combine these probabilities into a full joint distribution `p_GSR`:

```{r sprinkler_full_joint, echo=TRUE}
p_GSR <- tabListMult(
  list(p_G_SR, p_S_R, p_R)
)

ftable(p_GSR, row.vars = "GrassWet")
```


No suppose we know that the grass is wet. What is the probability it is
raining?


```{r sprinkler_grasswet_raining, echo=TRUE}
p_RG  <- tabMarg(p_GSR, c("Rain", "GrassWet"))  ## P(R,G)
p_G   <- tabMarg(p_RG, "GrassWet")              ## P(G)
p_R_G <- tabDiv(p_RG, p_G)                      ## P(R|G)
```

Reading across this CPT, we see that if the grass is wet, there is about a 36\%
chance it is due to rain.


While the above methods of manipulating CPTs works, the package `gRain`
provides us better functionality for answering questions such as this given
observed evidence.



# Genetic Inheritance

We now turn our attention to analysing genetic inheritance on the chromosomes
for a given DNA sequence.

An *allele* is the DNA sequence at a marker and can take two values marked
$A$ or $B$ (in practice there can be 10 or 20 different values).

A *genotype* is an unordered pair of alleles: $AA$, $AB$, or $BB$.

The genotype of a person at a specific marker is a random variable with state
space $\{AA, AB, BB\}$.

We are interested in the joint distribution of genotypes for a group of people.


```{r genetics_graph, echo=FALSE, results="hide"}
ab <- c("A", "B")

genetics_cptlist <- list(
    cptable(~father,                  levels = ab, values = c(1, 1)),
    cptable(~mother,                  levels = ab, values = c(1, 1)),
    cptable(~child + mother + father, levels = ab, values = c(1, 1, 1, 1, 1, 1, 1, 1))
    ) %>%
  compileCPT()

genetics_grain <- genetics_cptlist %>% grain()

genetics_grain %>% iplot()
```

We need to make a number of assumptions regarding genetic inheritance that we
list here:

  * A child inherits one allele from each parent independently.
  * The parent’s two alleles have equal probability of being passed on to the
    child.
  * Each combination has probability $0.25$; some lead to the same genotype for the
    child.


So in this case we have the the joint probability distribution as
being

\[
P(m, f, c) = P(m) \, P(f) \, P(c \mid m, f)
\]



```{r genotype_table, echo=FALSE}
genotypes <- c("AA", "AB", "BB");

calc_allele_prob <- function(child, mother, father) {
  child  <- strsplit(child,  "")[[1]]
  mother <- strsplit(mother, "")[[1]]
  father <- strsplit(father, "")[[1]]

  ## Probability of inheriting allele a from genotype gt
  P <- function(a, gt) ((a == gt[1]) + (a == gt[2])) / 2

  if(child[1] != child[2]) {
    P(child[1], mother) * P(child[2], father) + P(child[1], father) * P(child[2], mother)
  } else {
    P(child[1], mother) * P(child[2], father)
  }
}


prob_tbl <- expand_grid(
    child  = genotypes,
    mother = genotypes,
    father = genotypes
  ) %>%
  mutate(prob = pmap_dbl(list(child  = child,
                              mother = mother,
                              father = father),
                         calc_allele_prob)
         )

prob_tbl %>% print()
prob_tbl %>% glimpse()
```


Suppose we have a population frequency of alleles being 70\% $A$ and 30\% $B$.
We want to calculate the distribution of genotypes in the population.

This is a straightforward application of the Binomial distribution, and we use
the R function `dbinom()`


```{r calculate_genotype_distribution, echo=TRUE}
genotype_probs <- dbinom(0:2, size = 2, prob = 0.3)

print(genotype_probs)
```

Thus we have it that 49\% of the population have genotype $AA$, 42\% are $AB$
and 9\% are $BB$.

We now want to construct the CPTs for each of the nodes on the network:

```{r construct_genetic_cpts, echo=TRUE}
mother_cpt <- cptable(~ mother, values = genotype_probs, levels = genotypes)
print(mother_cpt)

father_cpt <- cptable(~ father, values = genotype_probs, levels = genotypes)
print(father_cpt)

p_inheritance <- prob_tbl %>%
  arrange(father, mother, child) %>%
  pull(prob)

child_cpt <- cptable(
  ~ child | mother + father,
  values = p_inheritance,
  levels = genotypes
  )

print(child_cpt)
```


Using the CPTs, we construct the Bayesian network:


```{r construct_genetic_network, echo=TRUE}
genetic_family_grain <- list(child_cpt, mother_cpt, father_cpt) %>%
  compileCPT() %>%
  grain()

plot(genetic_family_grain)
```


## Querying the Network

By using the `gRain` package it allows us to use the in-built functionality to
query the network.

To start with, we want to see the marginal distribution of alleles for the
father.

```{r query_father_marginal_distribution, echo=TRUE}
genetic_family_grain %>%
  querygrain(nodes = "father")
```

We can also look at the joint distribution of the mother and child:


```{r query_mother_child_joint_distribution, echo=TRUE}
genetic_family_grain %>%
  querygrain(
    nodes = c("child", "mother"),
    type  = "joint"
  )
```

We can pass this output to `ftable()` if we want:


```{r query_mother_child_ftable, echo=TRUE}
genetic_family_grain %>%
  querygrain(
    nodes = c("child", "mother"),
    type  = "joint"
    ) %>%
  ftable(col.vars = "child")
```


We can also produce the conditional distributions for each of the possible
values for the father given values of the mother and child.

```{r show_father_conditional_tables, echo=TRUE}
genetic_family_grain %>%
  querygrain(
    nodes = c("father", "child", "mother"),
    type  = "conditional"
    ) %>%
  ftable(col.vars = "father")
```


## Paternity Test

Now suppose we know that a child has genotype $AB$, and the mother has genotype
$BB$. Given that a man has genotype $AB$, what can we say about the chances of
the man being the father of the child?

This is a more complicated question than it sounds, as we need to think about
a few different ideas. We make use of the `pEvidence()` and `setEvidence()`.



```{r family_fmc_evidence, echo=TRUE}
p_fmc <- genetic_family_grain %>%
  setEvidence(
    evidence = list(mother = "BB", child = "AB", father = "AB")
    )

p_fmc %>% print()
p_fmc %>% pEvidence()
```

So we now have a probability for finding this particular combination of alleles
in a mother / father / child grouping.

So now we calculate the probability of any given man being $AB$?

```{r family_f_evidence, echo=TRUE}
p_f <- genetic_family_grain %>%
  setEvidence(evidence = list(father = "AB"))

p_f %>% print()
p_f %>% pEvidence()
```


Finally, we look at the probability of the child being $AB$ with a mother $BB$.

```{r family_mc_evidence, echo=TRUE}
p_mc <- genetic_family_grain %>%
  setEvidence(
    evidence = list(mother = "BB", child = "AB")
    )

p_mc %>% print()
p_mc %>% pEvidence()
```

So now we need to look at the various ratios of these probabilities to assess
probabilities.

```{r calculate_conditional_probabilities, echo=TRUE}
p_man <- pEvidence(p_fmc) / pEvidence(p_f)
print(p_man)

p_father <- p_man / pEvidence(p_mc)
print(p_father)
```


## Extended Genetic Family

We can now extend this model to add extended families, such as uncle/aunts and
grandparents.


```{r create_extended_family_network, echo=TRUE}
c_mf <- parray(
  varNames = c("child", "mother", "father"),
  levels   = rep(list(genotypes), 3),
  values   = p_inheritance
  )

f_gmgf <- parray(
  varNames = c("father", "grandmother", "grandfather"),
  levels   = rep(list(genotypes), 3),
  values = p_inheritance
  )

u_gmgf <- parray(
  varNames = c("uncle", "grandmother", "grandfather"),
  levels   = rep(list(genotypes), 3),
  values   = p_inheritance
  )

m  <- parray("mother",      levels = list(genotypes), values = genotype_probs)
gf <- parray("grandfather", levels = list(genotypes), values = genotype_probs)
gm <- parray("grandmother", levels = list(genotypes), values = genotype_probs)


extended_family_grain <- list(c_mf, m, f_gmgf, u_gmgf, gm, gf) %>%
  compileCPT() %>%
  grain()

extended_family_grain %>% iplot()
```


Now suppose the man is dead and we do not have any genetic information for him,
but we know his brother tests as $AA$. How does this affect the evidence for
potential paternity?

```{r family_uncle_evidence, echo=TRUE}
p_uncle <- extended_family_grain %>%
  setEvidence(evidence = list(mother = "BB", child = "AB", uncle = "AA"))

p_uncle %>% print()
p_uncle %>% pEvidence()
```

We also want to show how we calculate similar values by using `querygrain()`.

```{r family_uncle_querygrain, echo=TRUE}
extended_family_grain %>%
  querygrain(
    nodes = c("child", "mother", "uncle"),
    type  = "joint"
    ) %>%
  ftable(col.vars = "uncle")
```


# Chest-Clinic Network

We now turn our attention to the "Chest-Clinic Network" as an example of a 
Bayesian network.

Based on research by Lauritzen and Spiegelhalter, we construct a network based
on the following domain knowledge:


> Shortness-of-breath (dyspnoea) may be due to tuberculosis, lung cancer or
> bronchitis, or none of them, or more than one of them. A recent visit to Asia
> increases the chances of tuberculosis, while smoking is known to be a risk
> factor for both lung cancer and bronchitis.  The results of a single chest
> X-ray do not discriminate between lung cancer and tuberculosis, as neither
> does the presence or absence of dyspnoea.

We use this knowledge to construct a Bayesian network - breaking up the above
into discrete pieces of knowledge.


```{r construct_chest_clinic_dag, echo=TRUE}
chestclinic_dag <- list(
    "asia",
    c("tub", "asia"),
    "smoke",
    c("lung", "smoke"),
    c("bronc", "smoke"),
    c("either", "lung", "tub"),
    c("xray", "either"),
    c("dysp", "bronc", "either")
    ) %>%
  dag()

chestclinic_dag %>% plot()
```


## Constructing the Bayesian Network

```{r construct_chestclinic_bayesian_network, echo=TRUE}
a    <- cptable(~ asia,
                values = c(1, 99),
                levels = yn)
t_a  <- cptable(~ tub | asia,
                values = c(5, 95, 1, 99),
                levels = yn)
s    <- cptable(~ smoke,
                values = c(5, 5),
                levels = yn)
l_s  <- cptable(~ lung | smoke,
                values = c(1, 9, 1, 99),
                levels = yn)
b_s  <- cptable(~ bronc | smoke,
                values = c(6, 4, 3, 7),
                levels = yn)
e_lt <- cptable(~ either | lung + tub,
                values = c(1, 0, 1, 0, 1, 0, 0, 1),
                levels = yn)
x_e  <- cptable(~ xray | either,
                values = c(98, 2, 5, 95),
                levels = yn)
d_be <- cptable(~ dysp | bronc + either,
                values = c(9, 1, 7, 3, 8, 2, 1, 9),
                levels = yn)

chestclinic_cpt_grain <- list(a, t_a, s, l_s, b_s, e_lt, x_e, d_be) %>%
  compileCPT() %>%
  grain()

chestclinic_cpt_grain %>% summary()
```

We first should check some of the various conditional probabilities to ensure
we have built the network correctly:

```{r chestclinic_network_probability_check, echo=TRUE}
chestclinic_cpt_grain %>%
  querygrain(
    nodes = c("xray", "either"),
    type  = "conditional"
  )


chestclinic_cpt_grain %>%
  querygrain(
    nodes = c("dysp", "bronc", "either"),
    type  = "conditional"
    ) %>%
  ftable(row.vars = c("either", "bronc"))
```





## Building Networks from Data

Now that we have our network, we can use data to construct the CPTs. Assuming
no data is missing, we can use a tibble of data to set the probabilities.

```{r load_chest_sim_data, echo=TRUE}
data(chestSim500)
data(chestSim1000)
data(chestSim10000)


chestdata_500_tbl   <- chestSim500   %>% as_tibble()
chestdata_1000_tbl  <- chestSim1000  %>% as_tibble()
chestdata_10000_tbl <- chestSim10000 %>% as_tibble()

chestdata_500_tbl   %>% glimpse()
chestdata_1000_tbl  %>% glimpse()
chestdata_10000_tbl %>% glimpse()
```

We now can build multiple networks for each of the datasets we have.


```{r construct_chestclinic_grain_network, echo=TRUE}
chestsim_500_grain <- chestclinic_dag %>%
  grain(data = chestdata_500_tbl, smooth = 0.1) %>%
  compile(propagate = TRUE)

chestsim_1000_grain <- chestclinic_dag %>%
  grain(data = chestdata_1000_tbl, smooth = 0.1) %>%
  compile(propagate = TRUE)

chestsim_10000_grain <- chestclinic_dag %>%
  grain(data = chestdata_10000_tbl, smooth = 0.1) %>%
  compile(propagate = TRUE)
```

The unconditional probability of a patient having lung cancer should match
the proportion of cases in the dataset.

```{r calculate_500_unconditional_prob, echo=TRUE}
chestsim_500_grain %>%
  querygrain(nodes = "lung", type = "joint")

chestdata_500_tbl %>%
  count(lung, name = "count") %>%
  mutate(prob = count / sum(count))
```

We do the same thing for the other networks we constructed.

First we look at the 1,000 row dataset.

```{r calculate_1000_unconditional_prob, echo=TRUE}
chestsim_1000_grain %>%
  querygrain(nodes = "lung", type = "joint")

chestdata_1000_tbl %>%
  count(lung, name = "count") %>%
  mutate(prob = count / sum(count))
```


Now we look at the network based on 10,000 datapoints.

```{r calculate_10000_unconditional_prob, echo=TRUE}
chestsim_10000_grain %>%
  querygrain(nodes = "lung", type = "joint")

chestdata_10000_tbl %>%
  count(lung, name = "count") %>%
  mutate(prob = count / sum(count))
```


## Using Evidence

We can use given evidence to adjust the conditional probabilities.

Suppose the individual has visited Asia and has dyspnoea, what is the
conditional probability that the person has lung cancer?

```{r chestclinic_500_asia_dysp_query, echo=TRUE}
chestsim_500_grain %>%
  querygrain(
    nodes    = "lung",
    evidence = list(asia = "yes", dysp = "yes"),
    type     = "marginal"
    )
```

Given this evidence we see the probability of lung cancer has now increased to
around 6.5\%.

We repeat this exercise for the 1,000 network and see what probability it
shows.

```{r chestclinic_1000_asia_dysp_query, echo=TRUE}
chestsim_1000_grain %>%
  querygrain(
    nodes    = "lung",
    evidence = list(asia = "yes", dysp = "yes"),
    type     = "marginal"
    )
```


In this network, the probability is 10%.

```{r chestclinic_10000_asia_dysp_query, echo=TRUE}
chestsim_10000_grain %>%
  querygrain(
    nodes    = "lung",
    evidence = list(asia = "yes", dysp = "yes"),
    type     = "marginal"
    )
```

As before, the probability is about 10\%.


## Marginal Proportions for Diseases

We also want to calculate the different marginal probabilities for the three
conditions - Tuberculosis, Lung cancer and Dyspnoea - according to the
different

```{r calculate_marginal_proportions, echo=TRUE}
chestclinic_cpt_grain %>%
  querygrain(
    nodes = c("lung", "bronc", "tub"),
    type  = "marginal"
    )

chestsim_500_grain %>%
  querygrain(
    nodes = c("lung", "bronc", "tub"),
    type  = "marginal"
    )

chestsim_1000_grain %>%
  querygrain(
    nodes = c("lung", "bronc", "tub"),
    type  = "marginal"
    )

chestsim_10000_grain %>%
  querygrain(
    nodes = c("lung", "bronc", "tub"),
    type  = "marginal"
    )
```


# Scaling Up Bayesian Networks


All of the above approaches are example of the `Brute Force' approach
which is done by calculating the full joint distribution for the
network $p(V)$ as a multiple of the CPTs that comprise it.


$$
p(V) = p(a) \, p(t \mid a) \, p(s) \, p(l \mid s) \, p(b \mid s) \, p(e \mid t, l) \, p(d \mid e, b) \, p(x \mid e)
$$


This gives $p(V)$ represented by a table with $2^8 = 256$ entries.

We then marginalise and condition as desired to calculate whatever
probabilities we need.

The scaling of this approach is bad. A network with 80 variables, each with 10
values has a joint probability space of $10^{80}$, approximately the count of
atoms in the universe.


We are going to need a bigger boat...

So, we must find an approach that does not require the full joint distribution,
instead focusing on the the low dimensional CPTs and send `messages' between
them.

To use a network it first needs to be *compiled* and then *propagated*.

Compilation of a network based on CPTs is first *moralized* --- edges are added
between the parents of each node, and then directed edges are replaced with
undirected ones. It is then *triangulated* to form a triangulated graph.

The CPTs are transformed into *clique potentials* defined on the cliques of the
chordal graph.

We see this process as below:


```{r chestclinic_compile_process, echo=TRUE}
chestclinic_moralized    <- chestclinic_dag %>% moralize()
chestclinic_triangulated <- chestclinic_moralized %>% triangulate()

chestclinic_triangulated %>% plot()
```


Once we create the DAG, we view the triangulated data as a junction tree. The
messages passed between connected nodes on the graph involve the common
variables in the nodes, and propagating the information involves a double pass
down and up the tree.

```{r chestclinic_rip_plot, echo=FALSE}
chestclinic_triangulated %>%
  rip() %>%
  plot()
```

We now repeat the process from the above section, but now use the this approach
to calculate our probabilities.

```{r chestclinic_evidence_propagation, echo=TRUE}
chestsim500_ev_grain <- chestsim_500_grain %>%
  setFinding(nodes = "asia", states = "yes", propagate = FALSE) %>%
  setFinding(nodes = "dysp", states = "yes", propagate = FALSE) %>%
  propagate()
```

We now use this network to perform the same calculations. As a reminder, this
is calculating the marginal, joint and conditional probability for lung cancer
and bronchitis given that the person recently visited Asia and exhibited
shortness of breath (*dyspnoea*).


```{r chestclinic_evidence_probabilities, echo=TRUE}
chestsim500_ev_grain %>%
  querygrain(
    nodes = c("lung", "bronc"),
    type  = "marginal"
    )
  
chestsim500_ev_grain %>%
  querygrain(
    nodes = c("lung", "bronc"),
    type  = "joint"
    )
  
chestsim500_ev_grain %>%
  querygrain(
    nodes = c("lung", "bronc"),
    type  = "conditional"
    )
```


# Networks with Continuous Variables

All our Bayesian networks up to this point have focused on networks with
discrete variables, and we want to extend our knowledge to building and using
these networks.

For the purposes of this workshop, we will focus on Gaussian Bayesian networks:
networks where all the continuous variables are distributed according to
Gaussian densities (both univariate and multivariate) as well as the following
assumptions:

  * every node follows a normal distribution,
  * nodes without any parent, known as root nodes, are described by the
  respective marginal distributions,
  * the conditioning effect of the parent nodes is given by an additive linear
  term in the mean, and does not affect the variance,
  * the local distribution of each node can be equivalently expressed as a
  Gaussian linear model which includes an intercept and the node’s parents,
  without any interaction terms.


## Crop Analysis

We start with an example where we wish to analyse the crop yield of a
particular plant. To do this, we have a number of things to consider:

  1. The genetic and environmental potential of the plant
  1. Vegatative mass
  1. Harvest mass - the *crop*
  
For the first point, we construct two *latent* variables, $G$ and $E$ to
measure the *(G)enetic potential* and *(E)nvironmental potential* of the plant.

We also want to measure various aspects of the *(V)egatative mass*, as these
are indicators to the quality of the plant and hence affect the quality and
quantity of the crop yield. Thus, we also have the variable $V$.

The crop is a function of the vegatative mass and so is measured by the
*(N)umber of seeds* and their *mean (W)eight* - giving us variables $N$ and
$W$.

Finally, the *(C)rop* is determined by the number of seeds and the weight.

Converting all of the above into a Bayesian network, we can visualise it as
follows.

```{r construct_crop_bn_dag, echo=TRUE}
plant_crop_dag <- list(
    "G",
    "E",
    c("V", "G", "E"),
    c("N", "V"),
    c("W", "V"),
    c("C", "N", "W")
    ) %>%
  dag()

plant_crop_dag %>% plot()
```

If we were to specify a full joint probability distribution for $p$ variables,
this means we need $p$ means, $p$ variances and $\frac{1}{2}p(p-1)$ elements
of the correlation matrix and need to ensure that the correlation matrix is
positive definite.

Thus, this problem scales as $\mathcal{O}(N^2)$.

By using this graphical model, we only need to specify the various local,
conditional distributions, simplifying the problem.

### Setting Up the Network and Conditional Distributions

Before we add these distributions we want to construct the model.

```{r construct_plant_crop_network_model, echo=TRUE}
plant_crop_dagbnlearn <- model2network("[G][E][V|G:E][N|V][W|V][C|N:W]")

plant_crop_dagbnlearn %>% print()
```

Suppose for example, we specify the following conditional distributions:

\begin{eqnarray*}
G &\sim& \mathcal{N}(50, 10^2)                                             \\
E &\sim& \mathcal{N}(50, 10^2)                                             \\
V \mid G = g, E = e &\sim& \mathcal{N}(−10.35534 + 0.5 g + 0.70711 e, 5^2) \\
N \mid V = v &\sim& \mathcal{N}(45 + 0.1 v, 9.949874 ^ 2)                  \\
W \mid V = v &\sim& \mathcal{N}(15 + 0.7 v, 7.141428^2)                     \\
C \mid N = n, W = w &\sim& \mathcal{N}(0.3n + 0.7w, 6.25^2)
\end{eqnarray*}

We now set up these conditional distributions.

```{r gbn_conditional_dists, echo=TRUE}
disE <- list(coef = c("(Intercept)" = 50), sd = 10)
disG <- list(coef = c("(Intercept)" = 50), sd = 10)
disV <- list(coef = c("(Intercept)" = -10.35534, E = 0.70711, G = 0.5), sd = 5)
disN <- list(coef = c("(Intercept)" = 45, V = 0.1), sd = 9.949874)
disW <- list(coef = c("(Intercept)" = 15, V = 0.7), sd = 7.141428)
disC <- list(coef = c("(Intercept)" = 0, N = 0.3, W = 0.7), sd = 6.25)

dis_lst = list(E = disE, G = disG, V = disV, N = disN, W = disW, C = disC)


plant_crop_bnlearn <- custom.fit(plant_crop_dagbnlearn, dist = dis_lst)
```








# R Environment

```{r show_session_info, echo=TRUE, message=TRUE}
sessioninfo::session_info()
```
