---
title: "Dublin Data Science Workshop on Probabilistic Graphical Models"
author: "Mick Cooney <mickcooney@gmail.com>"
date: "Monday, 14 September 2020"
output:
  rmdformats::readthedown:
    fig_caption: yes
    theme: cerulean
    toc_depth: 3
    use_bookdown: yes

  html_document:
    fig_caption: yes
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float:
      smooth_scroll: FALSE

  pdf_document: default
---


```{r knit_opts, include = FALSE}
knitr::opts_chunk$set(tidy       = FALSE,
                      cache      = FALSE,
                      message    = FALSE,
                      warning    = FALSE,
                      fig.height =     8,
                      fig.width  =    11)

library(conflicted)
library(tidyverse)
library(magrittr)
library(scales)
library(cowplot)
library(gRain)
library(bnlearn)
library(rbmn)


source("custom_functions.R")

resolve_conflicts(c("magrittr", "rlang", "dplyr", "readr", "purrr", "ggplot2"))


options(
  width    = 80L,
  warn     = 1,
  mc.cores = parallel::detectCores()
  )



set.seed(42)

theme_set(theme_cowplot())
```



---


All code and data for this workshop is available at the following URL:

https://github.com/kaybenleroll/data_workshops

Code is available in the `ws_pgm_202009/` directory.



Content in this workshop is based on the book
[Probabalistic Graphical Models: Principles and Techniques](http://www.springer.com/us/book/9781461422983)
by Soren Hojsgaard.

<center>
![](img/graphical_models_cover.png)
</center>




Also look at the vignettes for the packages
[`gRain`](https://cran.r-project.org/web/packages/gRain/index.html)
and
[`gRbase`](https://cran.r-project.org/web/packages/gRbase/index.html)


Remember that this topic is massive. I could easily give a full
semester course on this stuff to really do it justice, so most of this
workshop is just me working through the material as I learn it.


As a result, it is highly likely this worksheet and code contains
typos, errors, logical flaws and other mistakes in need of correction
in this workshop, so if you note any, please let me know so I can try
to fix them!


If you want to look into this topic more, there is an old Coursera
course by Daphne Koller (tough going but excellent):

https://www.coursera.org/learn/probabilistic-graphical-models



This course was based on her textbook

[Probabalistic Graphical Models: Principles and Techniques](http://pgm.stanford.edu/)

<center>
![](img/pgm_koller_cover.png)
</center>


The Gaussian Graphical Models section of the workshop is based on material
found in the book "Bayes Networks With Examples in R" by Scutari and Denis.

<center>
![](img/bayesian_network.jpg)
</center>



# Basic Concepts

A graph is a mathematical object that can be defined as a pair

$$
\mathcal{G} = (V, E),
$$

where $V$ is a set of *vertices* or *nodes*, and $E$ is a set of *edges* that
joins two vertices. Edges in general may be directed, undirected or bidirected.
They are typically visualised by using shapes or points for the nodes and
lines for the edges.


The concept of *conditional independence* is related to that of
*statistical independence*. Suppose we have three random variables $A$, $B$ and
$C$, then $A$ and $B$ are *conditionally independent* given $C$, written

$$
A \perp B \mid C,
$$

iff, for every given value $c$ in $C$, $A$ and $B$ are independent in the
conditional distribution given $C = c$.


Another way of saying this is that for some $f$ a generic density or
probability mass function, then one characteristic of $A \perp B \, | \, C$
is that

$$
f(a, b \mid c) = f(a \mid c) f(b \mid c).
$$


An equivalent characterisation is that the joint density of $A$, $B$
and $C$ factorises as

$$
f(a, b, c) = g(a, c) \, h(b, c).
$$


Finally, we will also make heavy use of Bayes' Rule, the standard
formula for relating conditional probabilities:

$$
P(A \mid B) = \frac{P(A, B)}{P(B)} = \frac{P(B \mid A) P(A)}{P(B)}.
$$



## Conditional Probability

To explain the concept of conditional probability we first look at some basic
exercises in probability.

In the following questions, we are rolling two six-sided dice (denoted 2D6)
with $D_1$ and $D_2$ denoting the result of the first and second die
respectively, and we denote $T$ as being the sum of the two - i.e.

$$
T = D_1 + D_2
$$



*Question:* We start simple - with no further detail, what is the probability
of the total being 11?

*Answer:* We do not know anything about $D_1$ or $D_2$, so there are two
outcomes that result in $T = 11$:

$$(D_1 = 5, D_2 = 6) \text{ or } (D_1 = 6, D_2 = 5).$$

Thus, the probability of rolling 11 is

$$
P(T = 11) = \frac{2}{36} = 0.05556
$$

---

*Question:* What is the probability of getting 11 if the first dice is 5?

*Answer:* Out total is 11 if the second dice is 6, so there is only 1 possible
outcome of those six possible rolls.

$$
P(T = 11 \mid D_1 = 5) = \frac{1}{6} = 0.1667
$$

### Exercises

We now ask a few questions to ensure you understand these concepts.

  1. If the second dice is a 4, what is the probability the total was 7?
  1. What is the probability of the total being 10 or greater?
  1. If we know the first dice roll is 4, what is the probability of the total
     being 10 or greater?
  1. If we know the second dice roll is 2, what is the probability of the total
     being 10 or greater?
  1. What is the probability of the total being 3 or less?


## Conditional Dependence

Now suppose we know the totals of each dice instead? Suppose we know that
$T = 9$ and we have the following distribution for $D_1$:

$$
P(D_1) = \{0, \, 0, \, 0, \, 0, \, 0.5, \, 0.5 \}
$$
induces the following probability distribution for $D_2$:

$$
P(D_2) = \{0, \, 0, \, 0.5, \, 0.5, \, 0, \, 0 \}
$$

because $D_1 = 5 \implies D_2 = 4$ and $D_1 = 6 \implies D_2 = 3$


Thus, while we conceptually think of $D_1$ and $D_2$ as being independent of
one another, they become dependent if we have knowledge of $T$.

In this case we say that $D_1$ and $D_2$ are *conditionally dependent on one
another given* $T$.


Now we define new variables, $X_1$ and $X_2$:

$$
X_1 =
\begin{cases}
  1 \text{ iff } T \text{ is even}\\
  0 \text{ otherwise}
\end{cases}
\; \; \;
X_2 =
\begin{cases}
  1 \text{ iff } T >= 9\\
  0 \text{ otherwise}
\end{cases}
$$

Given no further information, we say that $X_1$ and $X_2$ are dependent, as
knowing information about one variables affects our knowledge of the other.

However, what happens if we know something about $T$?

In this case, we say that $X_1$ and $X_2$ are *independent given* $T$.



# Bayesian Network

We start with *Bayesian Networks* - where the nodes on the graph represent
discrete random variables.


## The Sprinkler Network

We start with a basic example of a Bayes Network: the sprinkler network.


```{r sprinkler_graph, echo=TRUE}
yn <- c("Yes", "No")

cptable_lst <- list(
  cptable(~Rain,                        levels = yn, values = c(2, 8)),
  cptable(~Sprinkler + Rain,            levels = yn, values = c(1, 99, 4, 6)),
  cptable(~wetGrass + Rain + Sprinkler, levels = yn, values = c(99, 1, 8, 2, 9, 1, 0, 1))
)

sprinkler_cptlist <- cptable_lst %>% compileCPT()
sprinkler_grain   <- sprinkler_cptlist %>% grain()

sprinkler_grain %>% plot()
```


Two events can cause grass to be wet: Either the sprinkler is on or it is
raining. Rain has a direct effect on the use of the sprinkler (namely that when
it rains, the sprinkler is usually not turned on).


This can be modeled with a Bayesian network. The variables (R)ain, (S)prinkler,
Wet(G)rass have two possible values: (y)es and (n)o.


We can factorise the joint probability mass function as

$$
p_{GSR}(g, s, r) = p_{G \mid SR}(g \mid s, r) p_{S \mid R}(s \mid r) p_R(r)
$$

or overloading the notation a little:

$$
P(G, S, R) = P(G \mid S, R) \; P(S, R) = P(G \mid S, R) \; P(S \mid R) \; P(R)
$$


This means we can construct the joint probability table by starting with the
$conditional probability tables$ (CPTs).


Create the 3 CPTs using the `parray()` function and the following conditional
probabilities:

\begin{align*}
P(R)     &= 0.2 \\
P(S|R)   &= 0.01 & P(S|\neg R)    &= 0.4\\
P(G|S,R) &= 0.99 & P(G|S, \neg R) &= 0.9 & P(G|\neg S, R) &= 0.8 & P(G|\neg S, \neg R) &= 0
\end{align*}


First we want to construct this network using our various conditional
probabilities:

```{r sprinkler_conditional_distributions, echo=TRUE}
yn <- c("yes", "no")

## P(R)
p_R <- parray(
  varNames = "Rain",
  levels = list(yn),
  values = c(0.2, 0.8)
)

## P(S|R)
p_S_R <- parray(
  varNames = c("Sprinkler", "Rain"),
  levels   = list(yn, yn), 
  values   = c(0.01, 0.99, 0.4, 0.6)
)

## P(G|S,R)
p_G_SR <- parray(
  varNames = c("GrassWet", "Sprinkler", "Rain"),
  levels   = list(yn, yn, yn),
  values   = c(0.99, 0.01, 0.8, 0.2, 0.9, 0.1, 0, 1)
)

ftable(p_G_SR, row.vars = "GrassWet")
```

We now combine these probabilities into a full joint distribution `p_GSR`:

```{r sprinkler_full_joint, echo=TRUE}
p_GSR <- tabListMult(
  list(p_G_SR, p_S_R, p_R)
)

ftable(p_GSR, row.vars = "GrassWet")
```


No suppose we know that the grass is wet. What is the probability it is
raining?


```{r sprinkler_grasswet_raining, echo=TRUE}
p_RG  <- tabMarg(p_GSR, c("Rain", "GrassWet"))  ## P(R,G)
p_G   <- tabMarg(p_RG, "GrassWet")              ## P(G)
p_R_G <- tabDiv(p_RG, p_G)                      ## P(R|G)
```

Reading across this CPT, we see that if the grass is wet, there is about a 36\%
chance it is due to rain.


While the above methods of manipulating CPTs works, the package `gRain`
provides us better functionality for answering questions such as this given
observed evidence.



# Genetic Inheritance

We now turn our attention to analysing genetic inheritance on the chromosomes
for a given DNA sequence.

An *allele* is the DNA sequence at a marker and can take two values marked
$A$ or $B$ (in practice there can be 10 or 20 different values).

A *genotype* is an unordered pair of alleles: $AA$, $AB$, or $BB$.

The genotype of a person at a specific marker is a random variable with state
space $\{AA, AB, BB\}$.

We are interested in the joint distribution of genotypes for a group of people.


```{r genetics_graph, echo=FALSE, results="hide"}
ab <- c("A", "B")

genetics_cptlist <- list(
    cptable(~father,                  levels = ab, values = c(1, 1)),
    cptable(~mother,                  levels = ab, values = c(1, 1)),
    cptable(~child + mother + father, levels = ab, values = c(1, 1, 1, 1, 1, 1, 1, 1))
    ) %>%
  compileCPT()

genetics_grain <- genetics_cptlist %>% grain()

genetics_grain %>% iplot()
```

We need to make a number of assumptions regarding genetic inheritance that we
list here:

  * A child inherits one allele from each parent independently.
  * The parent’s two alleles have equal probability of being passed on to the
    child.
  * Each combination has probability $0.25$; some lead to the same genotype for the
    child.


So in this case we have the the joint probability distribution as
being

\[
P(m, f, c) = P(m) \, P(f) \, P(c \mid m, f)
\]



```{r genotype_table, echo=FALSE}
genotypes <- c("AA", "AB", "BB");

calc_allele_prob <- function(child, mother, father) {
  child  <- strsplit(child,  "")[[1]]
  mother <- strsplit(mother, "")[[1]]
  father <- strsplit(father, "")[[1]]

  ## Probability of inheriting allele a from genotype gt
  P <- function(a, gt) ((a == gt[1]) + (a == gt[2])) / 2

  if(child[1] != child[2]) {
    P(child[1], mother) * P(child[2], father) + P(child[1], father) * P(child[2], mother)
  } else {
    P(child[1], mother) * P(child[2], father)
  }
}


prob_tbl <- expand_grid(
    child  = genotypes,
    mother = genotypes,
    father = genotypes
  ) %>%
  mutate(prob = pmap_dbl(list(child  = child,
                              mother = mother,
                              father = father),
                         calc_allele_prob)
         )

prob_tbl %>% print()
prob_tbl %>% glimpse()
```


Suppose we have a population frequency of alleles being 70\% $A$ and 30\% $B$.
We want to calculate the distribution of genotypes in the population.

This is a straightforward application of the Binomial distribution, and we use
the R function `dbinom()`


```{r calculate_genotype_distribution, echo=TRUE}
genotype_probs <- dbinom(0:2, size = 2, prob = 0.3)

print(genotype_probs)
```

Thus we have it that 49\% of the population have genotype $AA$, 42\% are $AB$
and 9\% are $BB$.

We now want to construct the CPTs for each of the nodes on the network:

```{r construct_genetic_cpts, echo=TRUE}
mother_cpt <- cptable(~ mother, values = genotype_probs, levels = genotypes)
print(mother_cpt)

father_cpt <- cptable(~ father, values = genotype_probs, levels = genotypes)
print(father_cpt)

p_inheritance <- prob_tbl %>%
  arrange(father, mother, child) %>%
  pull(prob)

child_cpt <- cptable(
  ~ child | mother + father,
  values = p_inheritance,
  levels = genotypes
  )

print(child_cpt)
```


Using the CPTs, we construct the Bayesian network:


```{r construct_genetic_network, echo=TRUE}
genetic_family_grain <- list(child_cpt, mother_cpt, father_cpt) %>%
  compileCPT() %>%
  grain()

plot(genetic_family_grain)
```


## Querying the Network

By using the `gRain` package it allows us to use the in-built functionality to
query the network.

To start with, we want to see the marginal distribution of alleles for the
father.

```{r query_father_marginal_distribution, echo=TRUE}
genetic_family_grain %>%
  querygrain(nodes = "father")
```

We can also look at the joint distribution of the mother and child:


```{r query_mother_child_joint_distribution, echo=TRUE}
genetic_family_grain %>%
  querygrain(
    nodes = c("child", "mother"),
    type  = "joint"
  )
```

We can pass this output to `ftable()` if we want:


```{r query_mother_child_ftable, echo=TRUE}
genetic_family_grain %>%
  querygrain(
    nodes = c("child", "mother"),
    type  = "joint"
    ) %>%
  ftable(col.vars = "child")
```


We can also produce the conditional distributions for each of the possible
values for the father given values of the mother and child.

```{r show_father_conditional_tables, echo=TRUE}
genetic_family_grain %>%
  querygrain(
    nodes = c("father", "child", "mother"),
    type  = "conditional"
    ) %>%
  ftable(col.vars = "father")
```


## Paternity Test

Now suppose we know that a child has genotype $AB$, and the mother has genotype
$BB$. Given that a man has genotype $AB$, what can we say about the chances of
the man being the father of the child?

This is a more complicated question than it sounds, as we need to think about
a few different ideas. We make use of the `pEvidence()` and `setEvidence()`.



```{r family_fmc_evidence, echo=TRUE}
p_fmc <- genetic_family_grain %>%
  setEvidence(
    evidence = list(mother = "BB", child = "AB", father = "AB")
    )

p_fmc %>% print()
p_fmc %>% pEvidence()
```

So we now have a probability for finding this particular combination of alleles
in a mother / father / child grouping.

So now we calculate the probability of any given man being $AB$?

```{r family_f_evidence, echo=TRUE}
p_f <- genetic_family_grain %>%
  setEvidence(evidence = list(father = "AB"))

p_f %>% print()
p_f %>% pEvidence()
```


Finally, we look at the probability of the child being $AB$ with a mother $BB$.

```{r family_mc_evidence, echo=TRUE}
p_mc <- genetic_family_grain %>%
  setEvidence(
    evidence = list(mother = "BB", child = "AB")
    )

p_mc %>% print()
p_mc %>% pEvidence()
```

So now we need to look at the various ratios of these probabilities to assess
probabilities.

```{r calculate_conditional_probabilities, echo=TRUE}
p_man <- pEvidence(p_fmc) / pEvidence(p_f)
print(p_man)

p_father <- p_man / pEvidence(p_mc)
print(p_father)
```


## Extended Genetic Family

We can now extend this model to add extended families, such as uncle/aunts and
grandparents.


```{r create_extended_family_network, echo=TRUE}
c_mf <- parray(
  varNames = c("child", "mother", "father"),
  levels   = rep(list(genotypes), 3),
  values   = p_inheritance
  )

f_gmgf <- parray(
  varNames = c("father", "grandmother", "grandfather"),
  levels   = rep(list(genotypes), 3),
  values = p_inheritance
  )

u_gmgf <- parray(
  varNames = c("uncle", "grandmother", "grandfather"),
  levels   = rep(list(genotypes), 3),
  values   = p_inheritance
  )

m  <- parray("mother",      levels = list(genotypes), values = genotype_probs)
gf <- parray("grandfather", levels = list(genotypes), values = genotype_probs)
gm <- parray("grandmother", levels = list(genotypes), values = genotype_probs)


extended_family_grain <- list(c_mf, m, f_gmgf, u_gmgf, gm, gf) %>%
  compileCPT() %>%
  grain()

extended_family_grain %>% iplot()
```


Now suppose the man is dead and we do not have any genetic information for him,
but we know his brother tests as $AA$. How does this affect the evidence for
potential paternity?

```{r family_uncle_evidence, echo=TRUE}
p_uncle <- extended_family_grain %>%
  setEvidence(evidence = list(mother = "BB", child = "AB", uncle = "AA"))

p_uncle %>% print()
p_uncle %>% pEvidence()
```

We also want to show how we calculate similar values by using `querygrain()`.

```{r family_uncle_querygrain, echo=TRUE}
extended_family_grain %>%
  querygrain(
    nodes = c("child", "mother", "uncle"),
    type  = "joint"
    ) %>%
  ftable(col.vars = "uncle")
```


# Chest-Clinic Network

We now turn our attention to the "Chest-Clinic Network" as an example of a 
Bayesian network.

Based on research by Lauritzen and Spiegelhalter, we construct a network based
on the following domain knowledge:


> Shortness-of-breath (dyspnoea) may be due to tuberculosis, lung cancer or
> bronchitis, or none of them, or more than one of them. A recent visit to Asia
> increases the chances of tuberculosis, while smoking is known to be a risk
> factor for both lung cancer and bronchitis.  The results of a single chest
> X-ray do not discriminate between lung cancer and tuberculosis, as neither
> does the presence or absence of dyspnoea.

We use this knowledge to construct a Bayesian network - breaking up the above
into discrete pieces of knowledge.


```{r construct_chest_clinic_dag, echo=TRUE}
chestclinic_dag <- list(
    "asia",
    c("tub", "asia"),
    "smoke",
    c("lung", "smoke"),
    c("bronc", "smoke"),
    c("either", "lung", "tub"),
    c("xray", "either"),
    c("dysp", "bronc", "either")
    ) %>%
  dag()

chestclinic_dag %>% plot()
```


## Constructing the Bayesian Network

```{r construct_chestclinic_bayesian_network, echo=TRUE}
a    <- cptable(~ asia,
                values = c(1, 99),
                levels = yn)
t_a  <- cptable(~ tub | asia,
                values = c(5, 95, 1, 99),
                levels = yn)
s    <- cptable(~ smoke,
                values = c(5, 5),
                levels = yn)
l_s  <- cptable(~ lung | smoke,
                values = c(1, 9, 1, 99),
                levels = yn)
b_s  <- cptable(~ bronc | smoke,
                values = c(6, 4, 3, 7),
                levels = yn)
e_lt <- cptable(~ either | lung + tub,
                values = c(1, 0, 1, 0, 1, 0, 0, 1),
                levels = yn)
x_e  <- cptable(~ xray | either,
                values = c(98, 2, 5, 95),
                levels = yn)
d_be <- cptable(~ dysp | bronc + either,
                values = c(9, 1, 7, 3, 8, 2, 1, 9),
                levels = yn)

chestclinic_cpt_grain <- list(a, t_a, s, l_s, b_s, e_lt, x_e, d_be) %>%
  compileCPT() %>%
  grain()

chestclinic_cpt_grain %>% summary()
```

We first should check some of the various conditional probabilities to ensure
we have built the network correctly:

```{r chestclinic_network_probability_check, echo=TRUE}
chestclinic_cpt_grain %>%
  querygrain(
    nodes = c("xray", "either"),
    type  = "conditional"
  )


chestclinic_cpt_grain %>%
  querygrain(
    nodes = c("dysp", "bronc", "either"),
    type  = "conditional"
    ) %>%
  ftable(row.vars = c("either", "bronc"))
```





## Building Networks from Data

Now that we have our network, we can use data to construct the CPTs. Assuming
no data is missing, we can use a tibble of data to set the probabilities.

```{r load_chest_sim_data, echo=TRUE}
data(chestSim500)
data(chestSim1000)
data(chestSim10000)


chestdata_500_tbl   <- chestSim500   %>% as_tibble()
chestdata_1000_tbl  <- chestSim1000  %>% as_tibble()
chestdata_10000_tbl <- chestSim10000 %>% as_tibble()

chestdata_500_tbl   %>% glimpse()
chestdata_1000_tbl  %>% glimpse()
chestdata_10000_tbl %>% glimpse()
```

We now can build multiple networks for each of the datasets we have.


```{r construct_chestclinic_grain_network, echo=TRUE}
chestsim_500_grain <- chestclinic_dag %>%
  grain(data = chestdata_500_tbl, smooth = 0.1) %>%
  compile(propagate = TRUE)

chestsim_1000_grain <- chestclinic_dag %>%
  grain(data = chestdata_1000_tbl, smooth = 0.1) %>%
  compile(propagate = TRUE)

chestsim_10000_grain <- chestclinic_dag %>%
  grain(data = chestdata_10000_tbl, smooth = 0.1) %>%
  compile(propagate = TRUE)
```

The unconditional probability of a patient having lung cancer should match
the proportion of cases in the dataset.

```{r calculate_500_unconditional_prob, echo=TRUE}
chestsim_500_grain %>%
  querygrain(nodes = "lung", type = "joint")

chestdata_500_tbl %>%
  count(lung, name = "count") %>%
  mutate(prob = count / sum(count))
```

We do the same thing for the other networks we constructed.

First we look at the 1,000 row dataset.

```{r calculate_1000_unconditional_prob, echo=TRUE}
chestsim_1000_grain %>%
  querygrain(nodes = "lung", type = "joint")

chestdata_1000_tbl %>%
  count(lung, name = "count") %>%
  mutate(prob = count / sum(count))
```


Now we look at the network based on 10,000 datapoints.

```{r calculate_10000_unconditional_prob, echo=TRUE}
chestsim_10000_grain %>%
  querygrain(nodes = "lung", type = "joint")

chestdata_10000_tbl %>%
  count(lung, name = "count") %>%
  mutate(prob = count / sum(count))
```


## Using Evidence

We can use given evidence to adjust the conditional probabilities.

Suppose the individual has visited Asia and has dyspnoea, what is the
conditional probability that the person has lung cancer?

```{r chestclinic_500_asia_dysp_query, echo=TRUE}
chestsim_500_grain %>%
  querygrain(
    nodes    = "lung",
    evidence = list(asia = "yes", dysp = "yes"),
    type     = "marginal"
    )
```

Given this evidence we see the probability of lung cancer has now increased to
around 6.5\%.

We repeat this exercise for the 1,000 network and see what probability it
shows.

```{r chestclinic_1000_asia_dysp_query, echo=TRUE}
chestsim_1000_grain %>%
  querygrain(
    nodes    = "lung",
    evidence = list(asia = "yes", dysp = "yes"),
    type     = "marginal"
    )
```


In this network, the probability is 10%.

```{r chestclinic_10000_asia_dysp_query, echo=TRUE}
chestsim_10000_grain %>%
  querygrain(
    nodes    = "lung",
    evidence = list(asia = "yes", dysp = "yes"),
    type     = "marginal"
    )
```

As before, the probability is about 10\%.


## Marginal Proportions for Diseases

We also want to calculate the different marginal probabilities for the three
conditions - Tuberculosis, Lung cancer and Dyspnoea - according to the
different

```{r calculate_marginal_proportions, echo=TRUE}
chestclinic_cpt_grain %>%
  querygrain(
    nodes = c("lung", "bronc", "tub"),
    type  = "marginal"
    )

chestsim_500_grain %>%
  querygrain(
    nodes = c("lung", "bronc", "tub"),
    type  = "marginal"
    )

chestsim_1000_grain %>%
  querygrain(
    nodes = c("lung", "bronc", "tub"),
    type  = "marginal"
    )

chestsim_10000_grain %>%
  querygrain(
    nodes = c("lung", "bronc", "tub"),
    type  = "marginal"
    )
```


# Additional Topics

Before we finish the topic of Bayesian networks with categorical variables,
there are a number of topics worth discussion briefly: in particular, how to
scale the model to larger numbers of variables and levels, and how to use the
data to determine possible configurations of the network in terms of
relationships between variables.


## Scaling Up Bayesian Networks

All of the above approaches are example of the `Brute Force' approach
which is done by calculating the full joint distribution for the
network $p(V)$ as a multiple of the CPTs that comprise it.


$$
p(V) = p(a) \, p(t \mid a) \, p(s) \, p(l \mid s) \, p(b \mid s) \, p(e \mid t, l) \, p(d \mid e, b) \, p(x \mid e)
$$


This gives $p(V)$ represented by a table with $2^8 = 256$ entries.

We then marginalise and condition as desired to calculate whatever
probabilities we need.

The scaling of this approach is bad. A network with 80 variables, each with 10
values has a joint probability space of $10^{80}$, approximately the count of
atoms in the universe.


We are going to need a bigger boat...

So, we must find an approach that does not require the full joint distribution,
instead focusing on the the low dimensional CPTs and send `messages' between
them.

To use a network it first needs to be *compiled* and then *propagated*.

Compilation of a network based on CPTs is first *moralized* --- edges are added
between the parents of each node, and then directed edges are replaced with
undirected ones. It is then *triangulated* to form a triangulated graph.

The CPTs are transformed into *clique potentials* defined on the cliques of the
chordal graph.

We see this process as below:


```{r chestclinic_compile_process, echo=TRUE}
chestclinic_moralized    <- chestclinic_dag %>% moralize()
chestclinic_triangulated <- chestclinic_moralized %>% triangulate()

chestclinic_triangulated %>% plot()
```


Once we create the DAG, we view the triangulated data as a junction tree. The
messages passed between connected nodes on the graph involve the common
variables in the nodes, and propagating the information involves a double pass
down and up the tree.

```{r chestclinic_rip_plot, echo=FALSE}
chestclinic_triangulated %>%
  rip() %>%
  plot()
```

We now repeat the process from the above section, but now use the this approach
to calculate our probabilities.

```{r chestclinic_evidence_propagation, echo=TRUE}
chestsim500_ev_grain <- chestsim_500_grain %>%
  setFinding(nodes = "asia", states = "yes", propagate = FALSE) %>%
  setFinding(nodes = "dysp", states = "yes", propagate = FALSE) %>%
  propagate()
```

We now use this network to perform the same calculations. As a reminder, this
is calculating the marginal, joint and conditional probability for lung cancer
and bronchitis given that the person recently visited Asia and exhibited
shortness of breath (*dyspnoea*).


```{r chestclinic_evidence_probabilities, echo=TRUE}
chestsim500_ev_grain %>%
  querygrain(
    nodes = c("lung", "bronc"),
    type  = "marginal"
    )
  
chestsim500_ev_grain %>%
  querygrain(
    nodes = c("lung", "bronc"),
    type  = "joint"
    )
  
chestsim500_ev_grain %>%
  querygrain(
    nodes = c("lung", "bronc"),
    type  = "conditional"
    )
```


## Learning Network Structure

One final topic to think about is using the data to infer a model to explain
it - so called *model selection* or *structural learning*.

Note that rather than using the data to determine the parameters of the model,
we instead use the data to infer the *structure* of the model, that is, what
the Bayesian network will look like.

For the chest clinic, we already know what the structure was, so we can
compare the output of the various structural learning approaches to see how
well it does.

Before we do that, we will plot the 'true' network:

```{r plot_reminder_chestclinic_dag, echo=TRUE}
chestclinic_dag %>% plot()
```

We now try a number of approaches to learn the structure - starting with the
*hill climbing* algorithm implemented via `hc()`.


```{r show_hillclimbing_500_dag, echo=TRUE}
chestSim500 %>%
  hc() %>%
  amat() %>%
  ggm::essentialGraph() %>%
  as("graphNEL") %>%
  plot()
```

We see that most of the network structure has been 'learned' from the data,
apart from the relationship between "asia" and "tub" - presumably as this
effect is small and thus weakly signalled within the data.

We repeat this exercise with `chestSim10000` as this may allow us to pick that
relationship out.

```{r show_hillclimbing_10000_dag, echo=TRUE}
chestSim10000 %>%
  hc() %>%
  amat() %>%
  ggm::essentialGraph() %>%
  as("graphNEL") %>%
  plot()
```

We can try one larger set of data with 100,000 entries.

```{r show_hillclimbing_100k_dag, echo=TRUE}
data(chestSim100000)

chestSim100000 %>%
  hc() %>%
  amat() %>%
  ggm::essentialGraph() %>%
  as("graphNEL") %>%
  plot()
```



# Networks with Continuous Variables

All our Bayesian networks up to this point have focused on networks with
discrete variables, and we want to extend our knowledge to building and using
these networks.

For the purposes of this workshop, we will focus on Gaussian Bayesian networks:
networks where all the continuous variables are distributed according to
Gaussian densities (both univariate and multivariate) as well as the following
assumptions:

  * every node follows a normal distribution,
  * nodes without any parent, known as root nodes, are described by the
  respective marginal distributions,
  * the conditioning effect of the parent nodes is given by an additive linear
  term in the mean, and does not affect the variance,
  * the local distribution of each node can be equivalently expressed as a
  Gaussian linear model which includes an intercept and the node’s parents,
  without any interaction terms.


## Crop Analysis

We start with an example where we wish to analyse the crop yield of a
particular plant. To do this, we have a number of things to consider:

  1. The genetic and environmental potential of the plant
  1. Vegatative mass
  1. Harvest mass - the *crop*
  
For the first point, we construct two *latent* variables, $G$ and $E$ to
measure the *(G)enetic potential* and *(E)nvironmental potential* of the plant.

We also want to measure various aspects of the *(V)egatative mass*, as these
are indicators to the quality of the plant and hence affect the quality and
quantity of the crop yield. Thus, we also have the variable $V$.

The crop is a function of the vegatative mass and so is measured by the
*(N)umber of seeds* and their *mean (W)eight* - giving us variables $N$ and
$W$.

Finally, the *(C)rop* is determined by the number of seeds and the weight.

Converting all of the above into a Bayesian network, we can visualise it as
follows:

```{r construct_crop_bn_dag, echo=FALSE, fig.cap="Plot of the Crop Network"}
plant_crop_dag <- list(
    "G",
    "E",
    c("V", "G", "E"),
    c("N", "V"),
    c("W", "V"),
    c("C", "N", "W")
    ) %>%
  dag()

plant_crop_dag %>% plot()
```

If we were to specify a full joint probability distribution for $p$ variables,
this means we need $p$ means, $p$ variances and $\frac{1}{2}p(p-1)$ elements
of the correlation matrix and need to ensure that the correlation matrix is
positive definite.

Thus, this problem scales as $\mathcal{O}(N^2)$.

By using this graphical model, we only need to specify the various local,
conditional distributions, simplifying the problem.


## Constructing the Network

Before we add these distributions we want to construct the model. We can
specify the above conditional factorisation of the model with the function
`model2network()` function from the `bnlearn` package.

```{r construct_plant_crop_network_model, echo=TRUE}
plant_crop_dagbnlearn <- model2network("[G][E][V|G:E][N|V][W|V][C|N:W]")

plant_crop_dagbnlearn %>% print()
```

Suppose for example, we specify the following conditional distributions:

\begin{eqnarray*}
G &\sim& \mathcal{N}(50, 10^2)                                             \\
E &\sim& \mathcal{N}(50, 10^2)                                             \\
V \mid G = g, E = e &\sim& \mathcal{N}(-10.35534 + 0.5 g + 0.70711 e, 5^2) \\
N \mid V = v &\sim& \mathcal{N}(45 + 0.1 v, 9.949874 ^ 2)                  \\
W \mid V = v &\sim& \mathcal{N}(15 + 0.7 v, 7.141428 ^ 2)                  \\
C \mid N = n, W = w &\sim& \mathcal{N}(0.3 n + 0.7 w, 6.25 ^ 2)
\end{eqnarray*}


We now set up these conditional distributions in `bnlearn`. From inspection we
see we are setting the parameters of a regression model at each node. As
discussed, each node is distributed according to a Gaussian density, with
conditional dependence manifesting as the conditional mean being a linear
function of the dependent variables.


```{r gbn_conditional_dists, echo=TRUE}
dist_E <- list(coef = c("(Intercept)" = 50), sd = 10)
dist_G <- list(coef = c("(Intercept)" = 50), sd = 10)
dist_V <- list(coef = c("(Intercept)" = -10.35534, E = 0.70711, G = 0.5), sd = 5)
dist_N <- list(coef = c("(Intercept)" = 45, V = 0.1), sd = 9.949874)
dist_W <- list(coef = c("(Intercept)" = 15, V = 0.7), sd = 7.141428)
dist_C <- list(coef = c("(Intercept)" = 0, N = 0.3, W = 0.7), sd = 6.25)

distrib_lst = list(E = dist_E, G = dist_G, V = dist_V, N = dist_N, W = dist_W,
                   C = dist_C)


plant_crop_bnlearn <- custom.fit(plant_crop_dagbnlearn, dist = distrib_lst)
```


## Inference on the Model

We now have constructed this 'toy' GBN and wish to focus on how we use this
network and model to answer questions about the system - much like we did
before in the discrete case.

Due to the use of Gaussian densities, we can use exact inference on this
network. This has the benefit of calculating exact conditional probabilities,
but imposes limits on the types of questions we can ask.

As a result, we will focus on methods of *approximate inference* - that is, the
use of Monte Carlo simulation to provide statistical estimates of the
parameters of interest.

We gain more flexibility in terms of the types of questions we can ask, at the
cost of producing answers that are only approximately correct.

That said, errors in this estimate are more often smaller than errors due to
the choice of model. In practice, "model error" is a much bigger concern.



### Naive Simulation

We start with a simple, but naive method of calculating these conditional
expectations: we create a sample of $N$ iterations from the realisations of
the joint distribution, and use this sample to calculation estimates of the
expectations.

```{r construct_plant_crop_sample, echo=TRUE}
n_sim <- 1000000

plant_crop_sample_tbl <- rbn(n = n_sim, plant_crop_bnlearn) %>% as_tibble()

plant_crop_sample_tbl %>% glimpse()
```


#### Marginal Distribution of C

We start with some simple questions: what is the marginal distribution of $C$?

```{r gbn_marginal_c, echo=TRUE}
ggplot(plant_crop_sample_tbl) +
  geom_histogram(aes(x = C), binwidth = 1) +
  scale_y_continuous(labels = label_comma()) +
  xlab("Value of C") +
  ylab("Simulation Count") +
  ggtitle("Marginal Distribution of C")
```

This is exactly what we would expect - we see the marginal distribution of $C$
follows that of a Gaussian density.


#### E[C | E = 50]

Now suppose we now that the crop was grown in an environment with average
potential - that is $E = 50$. What is the conditional distribution of $C$?

To do this, we filter the distribution for $E = 50$ and look at the resulting
values.

```{r gbn_e50_c_distribution_exact, echo=TRUE}
plant_crop_sample_tbl %>% filter(E == 50)
```

This approach needs to be amended, as we see that none of the samples has
$E = 50$ exactly. This is not an accident, for continuous distributions, the
probability mass of any single value is 0. Instead, we need to look at a
narrow range of values close to the value we care about.

```{r gbn_e50_c_distribution_width, echo=TRUE}
plant_crop_e50_5_tbl    <- plant_crop_sample_tbl %>% filter(abs(E - 50) < 5.0)
plant_crop_e50_1_tbl    <- plant_crop_sample_tbl %>% filter(abs(E - 50) < 1.0)
plant_crop_e50_01_tbl   <- plant_crop_sample_tbl %>% filter(abs(E - 50) < 0.1)
plant_crop_e50_001_tbl  <- plant_crop_sample_tbl %>% filter(abs(E - 50) < 0.01)

plant_crop_e50_tbl <- list(
    `Width 5.0`  = plant_crop_e50_5_tbl,
    `Width 1.0`  = plant_crop_e50_1_tbl,
    `Width 0.1`  = plant_crop_e50_01_tbl,
    `Width 0.01` = plant_crop_e50_001_tbl
    ) %>%
  bind_rows(.id = "width_label")

plot_tbl <- plant_crop_e50_tbl %>%
  group_by(width_label) %>%
  summarise(
    .groups = "drop",
    
    mean_value = mean(C),
    
    p10 = quantile(C, 0.10),
    p25 = quantile(C, 0.25),
    p50 = quantile(C, 0.50),
    p75 = quantile(C, 0.75),
    p90 = quantile(C, 0.90)
    )

ggplot(plot_tbl) +
  geom_errorbar(aes(x = width_label, ymin = p25, ymax = p75),
                width = 0, size = 3, colour = "red") +
  geom_errorbar(aes(x = width_label, ymin = p10, ymax = p90),
                width = 0, size = 1, colour = "black") +
  geom_point(aes(x = width_label, y = p50),        colour = "red") +
  geom_point(aes(x = width_label, y = mean_value), colour = "black") +
  expand_limits(y = 0) +
  xlab("Interval Width") +
  ylab("Value of C") +
  ggtitle("Conditional Values of C")
```

We can create a more direct comparison using facetted histograms:

```{r gbn_e50_c_distribution_facets, echo=TRUE}
ggplot(plant_crop_e50_tbl) +
  geom_histogram(aes(x = C), binwidth = 1) +
  facet_wrap(vars(width_label), scales = "free_y", ncol = 2) +
  xlab("C") +
  ylab("Frequency Count") +
  ggtitle("Facetted Histogram Plot")
```

This visualisation could be better so we instead try to use a density plot.

```{r gbn_e50_c_distribution_density, echo=TRUE}
ggplot(plant_crop_e50_tbl) +
  geom_line(aes(x = C, colour = width_label), stat = "density") +
  xlab("C") +
  ylab("Density") +
  ggtitle("Comparison Plot of Estimated Density Functions")
```


#### E[C | E = 70]

Now we repeat all of the above for $E = 70$.

```{r gbn_e70_c_distribution_width, echo=TRUE}
plant_crop_e70_tbl <- list(
    `Width 5.0`  = plant_crop_sample_tbl %>% filter(abs(E - 70) < 5.0),
    `Width 1.0`  = plant_crop_sample_tbl %>% filter(abs(E - 70) < 1.0),
    `Width 0.1`  = plant_crop_sample_tbl %>% filter(abs(E - 70) < 0.1),
    `Width 0.01` = plant_crop_sample_tbl %>% filter(abs(E - 70) < 0.01)
    ) %>%
  bind_rows(.id = "width_label")

plant_crop_e70_tbl %>% glimpse()
```


```{r gbn_e70_c_distribution_density, echo=TRUE}
ggplot(plant_crop_e70_tbl) +
  geom_line(aes(x = C, colour = width_label), stat = "density") +
  xlab("C") +
  ylab("Density") +
  ggtitle("Comparison Plot of Estimated Density Functions")
```


#### P(E, G | C > 80)

Now suppose we want to look at the environmental and genetic values associated
with very good crops (say $C > 80$).

```{r gbn_c_gt_80_distribution, echo=TRUE}
plant_crop_goodcrop_tbl <- plant_crop_sample_tbl %>%
  filter(C > 80)

ggplot(plant_crop_goodcrop_tbl) +
  geom_point(aes(x = E, y = G)) +
  xlab("E") +
  ylab("G") +
  ggtitle("Conditional Joint Distribution of E and G")
```


### Importance Sampling

While the simulation method works for the more commonly-occuring scenarios,
there is an issue for rare combinations of events as most of the generated
iterations will not be relevant for the analysis and the computation work is
wasted.

We use the `cpdist()` and `cpquery()` functions to allow us to do this work
without fully realising each of the iterations.

```{r construct_plant_crop_importance_sample, echo=TRUE}
plant_crop_importance_tbl <- cpdist(
    plant_crop_bnlearn,
    nodes    = c("E", "G"),
    evidence = (C > 80 & V < 40),
    n = 100000000
    ) %>%
  as_tibble()

plant_crop_importance_tbl %>% glimpse()
```

Now that we have generated the samples, we can plot both $E$ and $G$ variables.

```{r plot_e_g_importance_sampling, echo=TRUE}
ggplot(plant_crop_importance_tbl) +
  geom_point(aes(x = E, y = G)) +
  xlab("Environmental Potential") +
  ylab("Genetic Potential") +
  ggtitle("Plot of High Crop Yield Environmental Variables")
```

We can now compare these two scatterplots.

```{r plot_e_g_comparisons, echo=TRUE}
comparison_tbl <- list(
    simulation = plant_crop_goodcrop_tbl,
    importance = plant_crop_importance_tbl
    ) %>%
  bind_rows(.id = "source")

ggplot(comparison_tbl) +
  geom_point(aes(x = E, y = G, colour = source)) +
  xlab("Environmental Potential") +
  ylab("Genetic Potential") +
  ggtitle("Comparison Plot of (E,G) Where C > 80")
```

To use 'likelihood weighting' we need to set our `evidence` parameter in a
different way - we can set the values of the nodes directly as a list, or we
can set an upper and lower bound for a uniform set of values.

```{r construct_plant_crop_importance_lw_sample, echo=TRUE}
plant_crop_importance_lw_tbl <- cpdist(
    plant_crop_bnlearn,
    nodes    = c("E", "G"),
    evidence = list(C = c(80, 100), V = c(0, 40)),
    n        = 1000,
    method   = "lw"
    ) %>%
  as_tibble()

plant_crop_importance_tbl %>% glimpse()
```

We can produce a similar plot as before.

```{r plot_plant_crop_lw_scatter, echo=TRUE}
ggplot(plant_crop_importance_lw_tbl) +
  geom_point(aes(x = E, y = G)) +
  labs(x = "E", y = "G")
```


For further use of Bayesian Networks, the following packages are likely of
interest:

  * https://cran.r-project.org/package=bnlearn
  * https://cran.r-project.org/package=catnet
  * https://cran.r-project.org/package=deal
  * https://cran.r-project.org/package=pcalg
  * https://cran.r-project.org/package=gRbase
  * https://cran.r-project.org/package=gRain
  * https://cran.r-project.org/package=rbmn



# R Environment

```{r show_session_info, echo=TRUE, message=TRUE}
sessioninfo::session_info()
```
